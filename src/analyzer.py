"""
éšç§æ”¿ç­–åˆ†æå™¨ - æ ¸å¿ƒå®ç°ç¤ºä¾‹
åŸºäºæ–‡çŒ®ç»¼è¿°ä¸­çš„æ–¹æ³•è®º
"""

import spacy
from typing import List, Dict, Any, Set
import re
try:
    from semantic_analyzer import EnhancedSemanticAnalyzer
    ENHANCED_SEMANTIC_AVAILABLE = True
except ImportError:
    ENHANCED_SEMANTIC_AVAILABLE = False
    print("Warning: Enhanced semantic analyzer not available. Using basic analysis.")

try:
    from srl_extractor import SemanticRoleAnalyzer
    SRL_AVAILABLE = True
except ImportError:
    SRL_AVAILABLE = False
    print("Warning: SRL analyzer not available. Using basic parameter extraction.")

try:
    from transformer_srl import TransformerSRLExtractor
    TRANSFORMER_SRL_AVAILABLE = True
except ImportError:
    TRANSFORMER_SRL_AVAILABLE = False
    print("Warning: Transformer SRL not available.")

try:
    from llm_extractor import LLMExtractor
    LLM_AVAILABLE = True
except ImportError:
    LLM_AVAILABLE = False
    print("Warning: LLM extractor not available.")


class PrivacyPolicyAnalyzer:
    """
    éšç§æ”¿ç­–åˆ†æå™¨ä¸»ç±»
    
    å®ç°åŸºäºä»¥ä¸‹æ–¹æ³•ï¼š
    1. ä¾å­˜å¥æ³•è§£æ (Dependency Parsing)
    2. å‘½åå®ä½“è¯†åˆ« (NER)
    3. åŸºäºè§„åˆ™çš„æ¨¡å¼åŒ¹é…
    4. åŸºäºPIPEDAæ¡†æ¶çš„åˆ†ç±»
    """
    
    # PIPEDAçš„10ä¸ªå…¬å¹³ä¿¡æ¯åŸåˆ™
    PIPEDA_CATEGORIES = {
        "accountability": "é—®è´£æ€§",
        "identifying_purposes": "ç¡®å®šç›®çš„",
        "consent": "åŒæ„",
        "limiting_collection": "é™åˆ¶æ”¶é›†",
        "limiting_use": "é™åˆ¶ä½¿ç”¨ã€æŠ«éœ²å’Œä¿ç•™",
        "accuracy": "å‡†ç¡®æ€§",
        "safeguards": "å®‰å…¨ä¿éšœ",
        "openness": "å…¬å¼€æ€§",
        "individual_access": "ä¸ªäººè®¿é—®æƒ",
        "challenging_compliance": "è´¨ç–‘åˆè§„æ€§"
    }
    
    def __init__(self, model_name="en_core_web_sm", use_enhanced_semantic=True, use_srl=True,
                 use_transformer_srl=True, use_llm=False, llm_provider="deepseek", llm_api_key=None):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        å‚æ•°:
            model_name: spaCyæ¨¡å‹åç§°ï¼ˆéœ€è¦å…ˆä¸‹è½½: python -m spacy download en_core_web_smï¼‰
            use_enhanced_semantic: æ˜¯å¦ä½¿ç”¨å¢å¼ºè¯­ä¹‰åˆ†æï¼ˆé»˜è®¤Trueï¼‰
            use_srl: æ˜¯å¦ä½¿ç”¨spaCyè¯­ä¹‰è§’è‰²æ ‡æ³¨æå–å‚æ•°ï¼ˆé»˜è®¤Trueï¼‰
            use_transformer_srl: æ˜¯å¦ä½¿ç”¨Transformer SRLæå–å‚æ•°ï¼ˆé»˜è®¤Trueï¼Œæ¨èï¼‰
            use_llm: æ˜¯å¦ä½¿ç”¨LLMå¢å¼ºæå–ï¼ˆé»˜è®¤Falseï¼Œéœ€è¦API keyï¼‰
            llm_provider: LLMæä¾›å•† ("deepseek", "openai", "claude")
            llm_api_key: LLM APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œä¹Ÿå¯ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        """
        try:
            self.nlp = spacy.load(model_name)
        except OSError:
            print(f"æ¨¡å‹ {model_name} æœªæ‰¾åˆ°ã€‚è¯·è¿è¡Œ: python -m spacy download {model_name}")
            raise

        # åˆå§‹åŒ–å¢å¼ºè¯­ä¹‰åˆ†æå™¨
        self.use_enhanced_semantic = use_enhanced_semantic and ENHANCED_SEMANTIC_AVAILABLE
        if self.use_enhanced_semantic:
            self.enhanced_analyzer = EnhancedSemanticAnalyzer(self.nlp)
        else:
            self.enhanced_analyzer = None

        # åˆå§‹åŒ–spaCy SRLåˆ†æå™¨
        self.use_srl = use_srl and SRL_AVAILABLE
        if self.use_srl:
            print("ğŸ”§ Loading spaCy SRL analyzer...")
            self.srl_analyzer = SemanticRoleAnalyzer(self.nlp)
            print("   âœ“ spaCy SRL analyzer loaded")
        else:
            self.srl_analyzer = None

        # åˆå§‹åŒ–Transformer SRLåˆ†æå™¨
        self.use_transformer_srl = use_transformer_srl and TRANSFORMER_SRL_AVAILABLE
        if self.use_transformer_srl:
            print("ğŸ”§ Loading Transformer SRL analyzer...")
            self.transformer_srl = TransformerSRLExtractor()
            print("   âœ“ Transformer SRL analyzer loaded")
        else:
            self.transformer_srl = None

        # åˆå§‹åŒ–LLMå¢å¼ºå™¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤å…³é—­ï¼‰
        self.use_llm = use_llm and LLM_AVAILABLE
        if self.use_llm:
            print(f"ğŸ”§ Loading LLM extractor ({llm_provider})...")
            try:
                self.llm_extractor = LLMExtractor(provider=llm_provider, api_key=llm_api_key)
                print(f"   âœ“ LLM extractor loaded (è¾…åŠ©æ¨¡å¼)")
            except Exception as e:
                print(f"   âœ— LLM extractor failed: {e}")
                self.use_llm = False
                self.llm_extractor = None
        else:
            self.llm_extractor = None

        # æ·»åŠ è‡ªå®šä¹‰è§„åˆ™
        self._setup_matchers()
    
    def _setup_matchers(self):
        """è®¾ç½®æ¨¡å¼åŒ¹é…å™¨"""
        from spacy.matcher import Matcher
        
        self.matcher = Matcher(self.nlp.vocab)
        
        # æ¨¡å¼1: æ•°æ®æ”¶é›†
        # åŒ¹é… "collect/gather/obtain [data/information]"
        collection_pattern = [
            {"LEMMA": {"IN": ["collect", "gather", "obtain", "receive", "acquire"]}},
            {"POS": {"IN": ["DET", "PRON"]}, "OP": "?"},
            {"LOWER": {"IN": ["personal", "user", "your"]}, "OP": "?"},
            {"LOWER": {"IN": ["data", "information", "details", "content"]}}
        ]
        self.matcher.add("DATA_COLLECTION", [collection_pattern])
        
        # æ¨¡å¼2: æ•°æ®å…±äº«
        # åŒ¹é… "share/disclose/transfer [data] with/to [third party]"
        sharing_pattern = [
            {"LEMMA": {"IN": ["share", "disclose", "transfer", "provide", "send"]}},
            {"IS_SPACE": True, "OP": "*"},
            {"TEXT": {"REGEX": ".*"}, "OP": "*"},
            {"LOWER": {"IN": ["with", "to"]}},
            {"IS_SPACE": True, "OP": "*"},
            {"POS": {"IN": ["NOUN", "PROPN"]}}
        ]
        self.matcher.add("DATA_SHARING", [sharing_pattern])
        
        # æ¨¡å¼3: ç”¨æˆ·åŒæ„
        consent_pattern = [
            {"LOWER": {"IN": ["consent", "permission", "authorization", "agree", "accept"]}}
        ]
        self.matcher.add("CONSENT", [consent_pattern])

        # å¸¸è§çš„å¹²æ‰°å†…å®¹å…³é”®è¯ï¼ˆUIå…ƒç´ ã€å¯¼èˆªç­‰ï¼‰
        self.noise_keywords = {
            "click here", "learn more", "read more", "see more", "menu", "footer",
            "header", "navigation", "cookie settings", "settings", "home", "back",
            "next", "previous", "skip", "continue", "submit", "cancel", "close",
            "accept all", "reject all", "manage preferences", "sign in", "log in",
            "sign up", "register", "subscribe", "share", "print", "download",
            "search", "go", "ok", "yes", "no", "highlights", "explore the policy",
            "privacy policy", "terms of service", "read the full policy below",
            "return to top", "back to top", "go to top", "scroll to top"
        }

        # å¸¸è§çš„é¡µé¢å¯¼èˆªå’Œå…ƒæ•°æ®æ¨¡å¼
        self.noise_patterns = [
            r"^learn more",
            r"^read more",
            r"^see more",
            r"^click here",
            r"^explore",
            r"updated.*policy",
            r"effective\s+\w+\s+\d+,?\s+\d{4}$",  # "Effective June 26, 2024"
            r"^\d+$",  # çº¯æ•°å­—ï¼ˆè„šæ³¨ç¼–å·ï¼‰
            r"^\[\d+\]$",  # [1], [2] ç­‰
            r"^table of contents",
            r"^back to top",
            r"^privacy center",
        ]

    def is_noise_content(self, text: str) -> bool:
        """
        ä½¿ç”¨spaCyåˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºå¹²æ‰°å†…å®¹ï¼ˆçˆ¬è™«æŠ“å–çš„éæ”¿ç­–å†…å®¹ï¼‰

        å‚æ•°:
            text: å¾…æ£€æŸ¥çš„æ–‡æœ¬

        è¿”å›:
            Trueè¡¨ç¤ºæ˜¯å¹²æ‰°å†…å®¹ï¼Œåº”è¯¥è¿‡æ»¤
        """
        text_lower = text.lower().strip()

        # è§„åˆ™1: ç©ºæ–‡æœ¬æˆ–è¿‡çŸ­
        if len(text_lower) < 3:
            return True

        # è§„åˆ™2: å…¨æ˜¯æ•°å­—æˆ–ç‰¹æ®Šå­—ç¬¦
        if not any(c.isalpha() for c in text_lower):
            return True

        # è§„åˆ™3: æ£€æŸ¥æ˜¯å¦æ˜¯å¸¸è§UIå…ƒç´ ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
        if text_lower in self.noise_keywords:
            return True

        # è§„åˆ™3.5: ä½¿ç”¨æ­£åˆ™æ¨¡å¼åŒ¹é…å¸¸è§å™ªéŸ³
        for pattern in self.noise_patterns:
            if re.match(pattern, text_lower):
                return True

        # è§„åˆ™4: å…¨å¤§å†™çš„çŸ­æ–‡æœ¬ï¼ˆé€šå¸¸æ˜¯æ ‡é¢˜æˆ–æŒ‰é’®ï¼‰
        if text.isupper() and len(text.split()) <= 4:
            return True

        # è§„åˆ™5: ä½¿ç”¨spaCyè¿›è¡Œè¯­è¨€å­¦åˆ†æ
        doc = self.nlp(text)

        # è®¡ç®—æœ‰æ•ˆtokenæ•°ï¼ˆæ’é™¤ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
        valid_tokens = [t for t in doc if not t.is_space and not t.is_punct]
        num_tokens = len(valid_tokens)

        # å¤ªçŸ­ï¼ˆå°‘äº3ä¸ªæœ‰æ•ˆè¯ï¼‰
        if num_tokens < 3:
            return True

        # æ£€æŸ¥æ˜¯å¦æœ‰åŠ¨è¯
        has_verb = any(token.pos_ == "VERB" for token in valid_tokens)

        # æ£€æŸ¥æ˜¯å¦æœ‰åè¯
        has_noun = any(token.pos_ == "NOUN" or token.pos_ == "PROPN" for token in valid_tokens)

        # è§„åˆ™6: çŸ­å¥ä¸”æ²¡æœ‰åŠ¨è¯ï¼ˆé€šå¸¸æ˜¯å¯¼èˆªé“¾æ¥æˆ–æ ‡é¢˜ï¼‰
        if num_tokens < 5 and not has_verb:
            return True

        # è§„åˆ™7: æ²¡æœ‰åŠ¨è¯ä¹Ÿæ²¡æœ‰åè¯ï¼ˆå¯èƒ½æ˜¯æ— æ„ä¹‰ç‰‡æ®µï¼‰
        if not has_verb and not has_noun and num_tokens < 10:
            return True

        # è§„åˆ™8: æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰ˆæƒç¬¦å·æˆ–å¸¸è§é¡µè„šæ¨¡å¼
        if any(char in text for char in ['Â©', 'Â®', 'â„¢']) or text_lower.startswith('copyright'):
            return True

        # è§„åˆ™9: å•ä¸ªé—®å·æˆ–æ„Ÿå¹å·ï¼ˆå¯èƒ½æ˜¯UIå…ƒç´ ï¼‰
        if text.strip() in ['?', '!', '...']:
            return True

        # è§„åˆ™10: ç–‘é—®å¥å½¢å¼çš„æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯å¯¼èˆªç›®å½•ï¼‰
        # ä¾‹å¦‚ï¼š"What information do we collect?"
        # ä½†è¦ç¡®ä¿ç¡®å®æ˜¯ç®€çŸ­çš„ç–‘é—®å¥
        if text.strip().endswith('?') and num_tokens <= 10:
            # æ£€æŸ¥æ˜¯å¦åƒç›®å½•é¡¹ï¼ˆæ²¡æœ‰è¯¦ç»†è¯´æ˜ï¼‰
            if not any(word in text_lower for word in ['this', 'that', 'these', 'because', 'when', 'which']):
                return True

        # è§„åˆ™11: åªåŒ…å«äº§å“åç§°åˆ—è¡¨çš„è¡Œï¼ˆå¸¸è§äºäº§å“åˆ—è¡¨ï¼‰
        # ä¾‹å¦‚ï¼š"Facebook", "Instagram", "Messenger"
        if num_tokens <= 3 and all(token.pos_ == "PROPN" for token in valid_tokens):
            return True

        # è§„åˆ™12: é¡µè„šé“¾æ¥æ¨¡å¼ï¼ˆ"Policy" ç»“å°¾çš„çŸ­è¯­ï¼‰
        if text_lower.endswith('policy') and num_tokens <= 3:
            return True

        # è§„åˆ™13: åŒ…å«"privacy center"ç­‰å…ƒæ•°æ®å¼•ç”¨ï¼ˆä½†ä¸æ˜¯è§£é‡Šæ€§æ–‡å­—ï¼‰
        if 'privacy center' in text_lower:
            # "Learn more in Privacy Center" ç±»å‹çš„é“¾æ¥
            if num_tokens < 12:
                return True

        # è§„åˆ™14: åªæåˆ° "Privacy Policy" ä½†æ²¡æœ‰å®è´¨å†…å®¹
        if 'privacy policy' in text_lower:
            # å¦‚æœåªæ˜¯æ ‡é¢˜æˆ–é“¾æ¥ï¼ˆä¸åŒ…å« "this", "explains", "describes" ç­‰å®è´¨åŠ¨è¯ï¼‰
            if num_tokens < 8 and not any(word in text_lower for word in ['this', 'explains', 'describes', 'lets', 'helps']):
                return True

        # è§„åˆ™15: è§£é‡Šæ€§å°èŠ‚æ ‡é¢˜ï¼ˆæ²¡æœ‰å…·ä½“ä¿¡æ¯ï¼Œåªæ˜¯æè¿°æ€§çš„ï¼‰
        # ä¾‹å¦‚ï¼š"The feature we use it for, and how that feature works"
        if num_tokens <= 15 and not text.endswith('.'):
            # æ£€æŸ¥æ˜¯å¦æ˜¯æè¿°æ€§æ ‡é¢˜ï¼ˆåŒ…å«how/what/whyä½†æ²¡æœ‰å…·ä½“å†…å®¹ï¼‰
            if any(word in text_lower for word in ['how that', 'what that', 'why that']):
                return True
            # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ—è¡¨æ ‡é¢˜ï¼ˆåŒ…å«"for"ä½†å¾ˆçŸ­ï¼‰
            if text_lower.startswith(('the ', 'a ', 'an ')) and num_tokens < 12:
                # å¦‚æœæ²¡æœ‰å¥å·ä¸”åƒæ˜¯æ ‡é¢˜
                if not any(char in text for char in [';', '"', "'"]):
                    return True

        # è§„åˆ™16: ä»¥æ ‡ç‚¹ç¬¦å·å¼€å¤´çš„ç‰‡æ®µï¼ˆè‚¯å®šæ˜¯åˆ†æ®µé”™è¯¯ï¼‰
        if text_lower.startswith((',', ';', ':', 'and ', 'or ', 'but ')):
            return True

        return False

    def segment_policy(self, text: str) -> List[str]:
        """
        å°†éšç§æ”¿ç­–åˆ†æ®µï¼Œå¹¶è¿‡æ»¤å¹²æ‰°å†…å®¹

        å‚æ•°:
            text: å®Œæ•´çš„éšç§æ”¿ç­–æ–‡æœ¬

        è¿”å›:
            è¿‡æ»¤åçš„æ®µè½åˆ—è¡¨
        """
        # åˆ†æ®µï¼šé¦–å…ˆæŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²ï¼Œç„¶åå¯¹æ¯ä¸ªæ®µè½æŒ‰å•æ¢è¡Œç¬¦åˆ†å‰²
        paragraphs = []

        # å…ˆæŒ‰åŒæ¢è¡Œåˆ†å‰²å¤§æ®µè½
        large_paras = [p.strip() for p in text.split('\n\n') if p.strip()]

        # å¯¹æ¯ä¸ªå¤§æ®µè½ï¼Œå†æŒ‰å•æ¢è¡Œåˆ†å‰²
        for para in large_paras:
            # å¦‚æœåŒ…å«å•æ¢è¡Œç¬¦ï¼Œåˆ†å‰²æˆå¤šè¡Œ
            if '\n' in para:
                lines = [line.strip() for line in para.split('\n') if line.strip()]
                paragraphs.extend(lines)
            else:
                paragraphs.append(para)

        # è¿›ä¸€æ­¥æŒ‰å¥å­åˆ†å‰²ï¼ˆå¦‚æœæ®µè½å¤ªé•¿ï¼‰
        segments = []
        for para in paragraphs:
            # é¦–å…ˆæ£€æŸ¥æ•´ä¸ªæ®µè½æ˜¯å¦æ˜¯å¹²æ‰°å†…å®¹
            if self.is_noise_content(para):
                continue

            if len(para) > 500:  # å¦‚æœæ®µè½è¶…è¿‡500å­—ç¬¦
                doc = self.nlp(para)
                for sent in doc.sents:
                    # å¯¹æ¯ä¸ªå¥å­ä¹Ÿè¿›è¡Œå¹²æ‰°å†…å®¹æ£€æŸ¥
                    if not self.is_noise_content(sent.text):
                        segments.append(sent.text)
            else:
                segments.append(para)

        return segments
    
    def extract_privacy_parameters(self, doc) -> Dict[str, Any]:
        """
        ä»æ–‡æœ¬ä¸­æå–éšç§å‚æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰

        å‚æ•°:
            doc: spaCy Docå¯¹è±¡

        è¿”å›:
            åŒ…å«éšç§å‚æ•°çš„å­—å…¸
        """
        params = {
            "data_types": set(),
            "purposes": set(),
            "third_parties": set(),
            "retention_period": None,
            "user_rights": set(),
            "security_measures": set()
        }

        # ===== æ–¹æ³•1: ä½¿ç”¨spaCy SRLæå–ï¼ˆå¦‚æœå¯ç”¨ï¼‰ =====
        if self.use_srl and self.srl_analyzer:
            srl_params = self.srl_analyzer.extract_privacy_parameters(doc.text)

            # åˆå¹¶SRLæå–çš„å‚æ•°
            if srl_params.get("data_types"):
                params["data_types"].update(srl_params["data_types"])
            if srl_params.get("third_parties"):
                params["third_parties"].update(srl_params["third_parties"])
            if srl_params.get("purposes"):
                params["purposes"].update(srl_params["purposes"])
            if srl_params.get("methods"):
                params["security_measures"].update(srl_params["methods"])

        # ===== æ–¹æ³•1.5: ä½¿ç”¨Transformer SRLæå–ï¼ˆå¦‚æœå¯ç”¨ï¼Œæ¨èï¼‰ =====
        if self.use_transformer_srl and self.transformer_srl:
            transformer_params = self.transformer_srl.extract_privacy_parameters(doc.text)

            # åˆå¹¶Transformer SRLæå–çš„å‚æ•°
            if transformer_params.get("data_types"):
                params["data_types"].update(transformer_params["data_types"])
            if transformer_params.get("third_parties"):
                params["third_parties"].update(transformer_params["third_parties"])
            if transformer_params.get("purposes"):
                params["purposes"].update(transformer_params["purposes"])

        # ===== æ–¹æ³•1.6: ä½¿ç”¨LLMè¾…åŠ©æå–ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆçº§æœ€ä½ï¼‰ =====
        if self.use_llm and self.llm_extractor:
            try:
                llm_params = self.llm_extractor.extract_privacy_parameters(doc.text)

                # LLMä½œä¸ºè¾…åŠ©ï¼Œåªæ·»åŠ æœ¬åœ°æ¨¡å‹æœªå‘ç°çš„æ–°ä¿¡æ¯
                if llm_params.get("data_types"):
                    params["data_types"].update(llm_params["data_types"])
                if llm_params.get("third_parties"):
                    params["third_parties"].update(llm_params["third_parties"])
                if llm_params.get("purposes"):
                    params["purposes"].update(llm_params["purposes"])
            except Exception as e:
                # LLMå¤±è´¥ä¸å½±å“æ•´ä½“åˆ†æ
                pass

        # å¦‚æœå¯ç”¨äº†å¢å¼ºè¯­ä¹‰åˆ†æï¼Œå…ˆä½¿ç”¨å®ƒ
        if self.use_enhanced_semantic and self.enhanced_analyzer:
            enhanced_result = self.enhanced_analyzer.analyze_segment_enhanced(doc.text)
            
            # æå–å®Œæ•´çš„æ•°æ®ç±»å‹
            for dt_info in enhanced_result.get("data_types", []):
                params["data_types"].add(dt_info["text"])
                # ä¹Ÿæ·»åŠ è¯æ ¹å½¢å¼
                if dt_info.get("root") and dt_info["root"] != dt_info["text"]:
                    params["data_types"].add(dt_info["root"])
            
            # æå–è¯¦ç»†çš„ç›®çš„
            for purpose_info in enhanced_result.get("purposes", []):
                purpose_text = purpose_info["text"]
                params["purposes"].add(purpose_text)
                # å¦‚æœç›®çš„å¾ˆé•¿ï¼Œä¹Ÿæå–å…³é”®è¯
                if len(purpose_text.split()) > 3:
                    # æå–å…³é”®è¯ï¼ˆåè¯å’ŒåŠ¨è¯ï¼‰
                    purpose_doc = self.nlp(purpose_text)
                    keywords = [t.lemma_ for t in purpose_doc if t.pos_ in ["NOUN", "VERB"]]
                    params["purposes"].update(keywords)
            
            # ä»æ•°æ®-æ´»åŠ¨æ˜ å°„ä¸­æå–æ›´å¤šä¿¡æ¯
            for mapping in enhanced_result.get("data_activity_mappings", []):
                activity = mapping.get("activity", "")
                if activity:
                    params["purposes"].add(activity)
        
        # ç»§ç»­ä½¿ç”¨åŸæœ‰çš„åŸºç¡€æ–¹æ³•ä½œä¸ºè¡¥å……
        
        # 1. ä½¿ç”¨æ¨¡å¼åŒ¹é…å™¨
        matches = self.matcher(doc)
        for match_id, start, end in matches:
            span = doc[start:end]
            match_label = self.nlp.vocab.strings[match_id]
            
            if match_label == "DATA_COLLECTION":
                # æå–æ•°æ®ç±»å‹
                for token in span:
                    if token.pos_ == "NOUN":
                        params["data_types"].add(token.lemma_)
            
            elif match_label == "DATA_SHARING":
                # æå–ç¬¬ä¸‰æ–¹
                for token in span:
                    if token.pos_ in ["NOUN", "PROPN"] and token.dep_ in ["pobj", "dobj"]:
                        params["third_parties"].add(token.text)
        
        # 2. ä½¿ç”¨ä¾å­˜å¥æ³•åˆ†æ
        for token in doc:
            # è¯†åˆ«æ•°æ®æ”¶é›†åŠ¨è¯çš„å®¾è¯­
            if token.lemma_ in ["collect", "gather", "process", "use", "store"]:
                for child in token.children:
                    if child.dep_ == "dobj":
                        params["data_types"].add(child.lemma_)
                        # æŸ¥æ‰¾å¤åˆåè¯
                        for subchild in child.children:
                            if subchild.dep_ == "compound":
                                params["data_types"].add(f"{subchild.lemma_}_{child.lemma_}")
            
            # è¯†åˆ«ç›®çš„ - æå–æ›´è¯¦ç»†çš„çŸ­è¯­
            if token.lemma_ in ["for", "to"] and token.head.lemma_ in ["use", "process", "collect", "analyze", "provide", "improve"]:
                # æå–å®Œæ•´çš„ä»‹è¯çŸ­è¯­ä½œä¸ºç›®çš„
                purpose_span = None
                for child in token.children:
                    if child.pos_ in ["NOUN", "VERB", "PROPN"]:
                        # å°è¯•æå–å®Œæ•´çš„åè¯çŸ­è¯­
                        purpose_tokens = [child]
                        # æ”¶é›†ä¿®é¥°è¯å’Œå¤åˆè¯
                        for subchild in child.children:
                            if subchild.dep_ in ["amod", "compound", "prep"]:
                                purpose_tokens.append(subchild)
                        # æ„å»ºç›®çš„çŸ­è¯­
                        purpose_text = " ".join([t.text for t in sorted(purpose_tokens, key=lambda x: x.i)])
                        if len(purpose_text) > 2:
                            params["purposes"].add(purpose_text.lower())
                        # ä¹Ÿæ·»åŠ å•ä¸ªè¯ä½œä¸ºfallback
                        params["purposes"].add(child.lemma_)
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å­è¯ï¼Œå°è¯•æå–æ•´ä¸ªä»‹è¯çŸ­è¯­
                if not any(child.pos_ in ["NOUN", "VERB"] for child in token.children):
                    # æå–"for/to"åé¢çš„å®Œæ•´çŸ­è¯­
                    start_idx = token.i
                    end_idx = min(start_idx + 5, len(doc))  # æœ€å¤š5ä¸ªè¯
                    if end_idx > start_idx + 1:
                        purpose_span = doc[start_idx + 1:end_idx]
                        purpose_text = purpose_span.text.strip()
                        if len(purpose_text) > 2 and len(purpose_text) < 50:
                            params["purposes"].add(purpose_text.lower())
        
        # é¢å¤–æå–ï¼šè¯†åˆ«å¸¸è§çš„æ´»åŠ¨æ¨¡å¼
        activity_patterns = [
            r"to\s+(?:provide|deliver|offer|enable|support|improve|enhance|personalize|customize)\s+([^.,]+)",
            r"for\s+(?:marketing|advertising|analytics|research|development|service|operation|security|compliance)\s*([^.,]*)",
            r"(?:when|while|during)\s+(?:you|users?)\s+(?:use|access|visit|browse|interact|purchase|register|sign)\s+([^.,]+)",
        ]
        import re
        text_lower = doc.text.lower()
        for pattern in activity_patterns:
            matches = re.finditer(pattern, text_lower)
            for match in matches:
                activity = match.group(1).strip() if match.lastindex else match.group(0).strip()
                if activity and len(activity) > 2 and len(activity) < 50:
                    params["purposes"].add(activity)
        
        # ===== åŸºäºspaCyæ¨¡å‹èƒ½åŠ›çš„æ•°æ®ç±»å‹æå– =====
        # æ–¹æ³•1: åˆ©ç”¨subtreeæå–æ”¶é›†åŠ¨è¯çš„å®Œæ•´å®¾è¯­
        collection_verbs = {"collect", "gather", "obtain", "receive", "acquire", "capture", "store", "process", "use"}
        
        for token in doc:
            if token.lemma_ in collection_verbs:
                # æ‰¾åˆ°ç›´æ¥å®¾è¯­ï¼ˆdobjï¼‰
                for child in token.children:
                    if child.dep_ == "dobj":
                        # ä½¿ç”¨spaCyçš„subtreeåŠŸèƒ½ - è‡ªåŠ¨åŒ…å«æ‰€æœ‰ä¾èµ–è¯
                        subtree_tokens = list(child.subtree)
                        phrase = " ".join([t.text for t in subtree_tokens])
                        # æ¸…ç†å¼•ç”¨æ ‡è®°
                        phrase = re.sub(r'\[\d+\]', '', phrase).strip()
                        if self._is_valid_data_type(phrase):
                            params["data_types"].add(phrase.lower())
                        
                        # æŸ¥æ‰¾å¹¶åˆ—ç»“æ„ï¼ˆconjä¾èµ–ï¼‰- spaCyè‡ªåŠ¨è¯†åˆ«
                        for conj_child in child.children:
                            if conj_child.dep_ == "conj":
                                conj_subtree = list(conj_child.subtree)
                                conj_phrase = " ".join([t.text for t in conj_subtree])
                                conj_phrase = re.sub(r'\[\d+\]', '', conj_phrase).strip()
                                if self._is_valid_data_type(conj_phrase):
                                    params["data_types"].add(conj_phrase.lower())
        
        # æ–¹æ³•2: åˆ©ç”¨noun_chunksåœ¨æ”¶é›†ä¸Šä¸‹æ–‡ä¸­æå–
        # spaCyè‡ªåŠ¨è¯†åˆ«æ‰€æœ‰åè¯çŸ­è¯­å—
        collection_verb_indices = {i for i, token in enumerate(doc) if token.lemma_ in collection_verbs}
        
        for chunk in doc.noun_chunks:
            # æ£€æŸ¥chunkæ˜¯å¦åœ¨åŒ…å«æ”¶é›†åŠ¨è¯çš„å¥å­ä¸­
            chunk_sent = None
            for sent in doc.sents:
                if chunk.start >= sent.start and chunk.end <= sent.end:
                    chunk_sent = sent
                    break
            
            if chunk_sent:
                # æ£€æŸ¥å¥å­ä¸­æ˜¯å¦æœ‰æ”¶é›†åŠ¨è¯
                sent_has_collection = any(i in collection_verb_indices 
                                         for i in range(chunk_sent.start, chunk_sent.end))
                
                if sent_has_collection:
                    # æ£€æŸ¥chunkçš„æ ¹æ˜¯å¦ä¾èµ–äºæ”¶é›†åŠ¨è¯ï¼ˆåˆ©ç”¨headé“¾ï¼‰
                    chunk_root = chunk.root
                    for verb_idx in collection_verb_indices:
                        if chunk_sent.start <= verb_idx < chunk_sent.end:
                            verb_token = doc[verb_idx]
                            # æ£€æŸ¥ä¾èµ–å…³ç³»
                            if self._is_dependent_of(chunk_root, verb_token, doc):
                                phrase = chunk.text.lower()
                                phrase = re.sub(r'\[\d+\]', '', phrase).strip()
                                if self._is_valid_data_type(phrase):
                                    params["data_types"].add(phrase)
        
        # æ–¹æ³•3: åˆ©ç”¨ä¾å­˜è§£ææå–"like/such as"å¼•å¯¼çš„åˆ—è¡¨
        for token in doc:
            # spaCyè‡ªåŠ¨è¯†åˆ«"like"ä½œä¸ºprep
            if token.lemma_ == "like" or (token.text.lower() == "as" and token.i > 0 
                                         and doc[token.i-1].lemma_ == "such"):
                # æ‰¾åˆ°pobjï¼ˆä»‹è¯å®¾è¯­ï¼‰
                for child in token.children:
                    if child.dep_ == "pobj":
                        # æå–pobjçš„subtreeï¼ˆåŒ…å«æ‰€æœ‰å¹¶åˆ—é¡¹ï¼‰
                        pobj_subtree = list(child.subtree)
                        # æå–æ‰€æœ‰åè¯ï¼ˆåˆ©ç”¨spaCyçš„POSæ ‡æ³¨ï¼‰
                        for t in pobj_subtree:
                            if t.pos_ == "NOUN":
                                # æå–è¯¥åè¯çš„å®Œæ•´çŸ­è¯­
                                noun_chunk = None
                                for chunk in doc.noun_chunks:
                                    if t.i >= chunk.start and t.i < chunk.end:
                                        noun_chunk = chunk
                                        break
                                if noun_chunk:
                                    phrase = noun_chunk.text.lower()
                                    phrase = re.sub(r'\[\d+\]', '', phrase).strip()
                                    if self._is_valid_data_type(phrase):
                                        params["data_types"].add(phrase)
                        
                        # æ£€æŸ¥å¹¶åˆ—ç»“æ„ï¼ˆconjï¼‰- spaCyè‡ªåŠ¨è¯†åˆ«
                        for conj in child.children:
                            if conj.dep_ == "conj":
                                for t in conj.subtree:
                                    if t.pos_ == "NOUN":
                                        noun_chunk = None
                                        for chunk in doc.noun_chunks:
                                            if t.i >= chunk.start and t.i < chunk.end:
                                                noun_chunk = chunk
                                                break
                                        if noun_chunk:
                                            phrase = noun_chunk.text.lower()
                                            phrase = re.sub(r'\[\d+\]', '', phrase).strip()
                                            if self._is_valid_data_type(phrase):
                                                params["data_types"].add(phrase)
        for ent in doc.ents:
            if ent.label_ == "ORG":
                # ç»„ç»‡å¯èƒ½æ˜¯ç¬¬ä¸‰æ–¹
                params["third_parties"].add(ent.text)
            elif ent.label_ == "DATE":
                # å¯èƒ½æ˜¯æ•°æ®ä¿ç•™æœŸ
                if not params["retention_period"]:
                    params["retention_period"] = ent.text
        
        # 4. è¯†åˆ«ç”¨æˆ·æƒåˆ©ç›¸å…³è¯æ±‡
        rights_keywords = {
            "access", "correct", "delete", "withdraw", "opt-out", 
            "unsubscribe", "export", "portability"
        }
        for token in doc:
            if token.lemma_ in rights_keywords:
                params["user_rights"].add(token.lemma_)
        
        # 5. è¯†åˆ«å®‰å…¨æªæ–½
        security_keywords = {
            "encrypt", "secure", "protect", "safeguard", "ssl", 
            "https", "firewall", "authentication"
        }
        for token in doc:
            if token.lemma_ in security_keywords or token.text.lower() in security_keywords:
                params["security_measures"].add(token.text)
        
        # æ¸…ç†å’Œè¿‡æ»¤æ•°æ®ç±»å‹
        cleaned_data_types = set()
        for dt in params["data_types"]:
            cleaned = self._clean_data_type(dt)
            if cleaned and self._is_valid_data_type(cleaned):
                # ç»Ÿä¸€è½¬æ¢ä¸ºå°å†™ä»¥é¿å…é‡å¤ï¼ˆå¦‚ "Apps" vs "apps"ï¼‰
                cleaned_data_types.add(cleaned.lower())
        params["data_types"] = cleaned_data_types

        # åŒæ ·å¯¹ç¬¬ä¸‰æ–¹å’Œç›®çš„è¿›è¡Œå°å†™å½’ä¸€åŒ–
        params["third_parties"] = {tp.lower() for tp in params["third_parties"] if tp}
        params["purposes"] = {p.lower() for p in params["purposes"] if p}

        # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
        return {k: sorted(list(v)) if isinstance(v, set) else v for k, v in params.items()}
    
    def _is_dependent_of(self, token, ancestor, doc, max_depth=5):
        """æ£€æŸ¥tokenæ˜¯å¦ä¾èµ–äºancestorï¼ˆåˆ©ç”¨spaCyçš„headé“¾ï¼‰"""
        current = token
        depth = 0
        while current != ancestor and current.head != current and depth < max_depth:
            current = current.head
            depth += 1
            if current == ancestor:
                return True
        return False
    
    def _extract_example_list(self, obj_token, doc, params):
        """æå–"like"æˆ–"such as"å¼•å¯¼çš„ç¤ºä¾‹åˆ—è¡¨ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
        # æŸ¥æ‰¾obj_tokenåé¢çš„"like"æˆ–"such as"
        sentence_end = len(doc)
        for sent in doc.sents:
            if obj_token.i >= sent.start and obj_token.i < sent.end:
                sentence_end = sent.end
                break
        
        for i in range(obj_token.i + 1, min(obj_token.i + 20, sentence_end)):
            token = doc[i]
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯"like"æˆ–"such as"
            is_like = token.lemma_ == "like"
            is_such_as = (token.text.lower() == "as" and i > 0 and doc[i-1].lemma_ == "such")
            
            if is_like or is_such_as:
                # æå–åç»­çš„åˆ—è¡¨é¡¹
                list_start = i + 1
                list_end = min(list_start + 20, sentence_end)
                list_text_tokens = []
                
                # æ”¶é›†åˆ—è¡¨æ–‡æœ¬
                for j in range(list_start, list_end):
                    t = doc[j]
                    if t.is_punct and t.text in [".", ";", "\n"]:
                        break
                    # è·³è¿‡å¼•ç”¨æ ‡è®°å¦‚[7]
                    if t.text.startswith('[') and t.text.endswith(']'):
                        continue
                    list_text_tokens.append(t.text)
                
                # æ„å»ºåˆ—è¡¨æ–‡æœ¬
                list_text = " ".join(list_text_tokens)
                
                # æ¸…ç†å’Œåˆ†å‰²åˆ—è¡¨é¡¹
                list_text = re.sub(r'\[\d+\]', '', list_text).strip()
                
                # åˆ†å‰²åˆ—è¡¨é¡¹ï¼ˆå¤„ç† "X, Y, or Z" æ ¼å¼ï¼‰
                items = []
                parts = re.split(r'\s*,\s*', list_text)
                for part in parts:
                    part = part.strip()
                    # å¤„ç†"or X"
                    if re.match(r'^or\s+', part, re.IGNORECASE):
                        part = re.sub(r'^or\s+', '', part, flags=re.IGNORECASE)
                    
                    # å¦‚æœpartåŒ…å«"or"ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
                    if ' or ' in part.lower():
                        or_items = re.split(r'\s+or\s+', part, flags=re.IGNORECASE)
                        items.extend([item.strip() for item in or_items])
                    else:
                        items.append(part)
                
                # æ·»åŠ æœ‰æ•ˆçš„æ•°æ®ç±»å‹
                for item in items:
                    item = item.strip()
                    if self._is_valid_data_type(item) and len(item) > 2:
                        params["data_types"].add(item.lower())
                
                break
    
    def _extract_conjunction_items(self, token, doc) -> List[str]:
        """æå–å¹¶åˆ—ç»“æ„ä¸­çš„é¡¹ï¼ˆX, Y, and Zï¼‰"""
        items = [token.text]
        
        # æŸ¥æ‰¾å¹¶åˆ—è¿è¯
        for child in token.children:
            if child.dep_ == "conj":
                items.append(child.text)
                # é€’å½’æŸ¥æ‰¾æ›´å¤šå¹¶åˆ—é¡¹
                items.extend(self._extract_conjunction_items(child, doc))
        
        # æŸ¥æ‰¾"and"æˆ–"or"è¿æ¥çš„é¡¹
        if token.head.pos_ == "NOUN":
            for sibling in token.head.children:
                if sibling.dep_ == "conj" and sibling.pos_ == "NOUN":
                    items.append(sibling.text)
        
        return items
    
    def _is_valid_data_type(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ•°æ®ç±»å‹"""
        if not text or len(text) < 2:
            return False
        
        text_lower = text.lower().strip()
        
        # è¿‡æ»¤æ‰å™ªéŸ³
        noise_words = {
            "we", "you", "your", "our", "they", "them", "it", "this", "that",
            "these", "those", "i", "me", "my", "he", "she", "him", "her",
            "]", "[", "(", ")", "{", "}", ".", ",", ";", ":", "!", "?",
            "the", "a", "an", "is", "are", "was", "were", "be", "been",
            "have", "has", "had", "do", "does", "did", "will", "would",
            "can", "could", "may", "might", "must", "should"
        }
        
        if text_lower in noise_words:
            return False
        
        # è¿‡æ»¤æ‰çº¯æ ‡ç‚¹
        if not any(c.isalnum() for c in text):
            return False
        
        # è¿‡æ»¤æ‰å•ä¸ªå­—ç¬¦ï¼ˆé™¤éæ˜¯ç‰¹æ®Šçš„æ•°æ®ç±»å‹ç¼©å†™ï¼‰
        if len(text_lower) == 1 and text_lower not in ["id", "ip"]:
            return False
        
        return True
    
    def _clean_data_type(self, text: str) -> str:
        """æ¸…ç†æ•°æ®ç±»å‹æ–‡æœ¬"""
        if not text:
            return ""
        
        # ç§»é™¤å‰åç©ºæ ¼
        text = text.strip()
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆä¿ç•™è¿å­—ç¬¦ï¼‰
        import re
        text = re.sub(r'[^\w\s-]', '', text)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def classify_category(self, text: str, params: Dict[str, Any]) -> str:
        """
        å°†æ–‡æœ¬æ®µè½åˆ†ç±»åˆ°PIPEDAç±»åˆ«
        
        å‚æ•°:
            text: æ–‡æœ¬æ®µè½
            params: æå–çš„éšç§å‚æ•°
            
        è¿”å›:
            PIPEDAç±»åˆ«
        """
        text_lower = text.lower()
        
        # åŸºäºè§„åˆ™çš„åˆ†ç±»
        if any(word in text_lower for word in ["collect", "gather", "obtain", "receive"]):
            if len(params["data_types"]) > 0:
                return "limiting_collection"
        
        if any(word in text_lower for word in ["consent", "permission", "agree", "accept"]):
            return "consent"
        
        if any(word in text_lower for word in ["share", "disclose", "transfer", "third party", "partner"]):
            return "limiting_use"
        
        if any(word in text_lower for word in ["secure", "protect", "encrypt", "safeguard"]):
            return "safeguards"
        
        if any(word in text_lower for word in ["access", "correct", "delete", "right"]):
            return "individual_access"
        
        if any(word in text_lower for word in ["purpose", "use for", "used to"]):
            return "identifying_purposes"
        
        if any(word in text_lower for word in ["accurate", "update", "correct"]):
            return "accuracy"
        
        if any(word in text_lower for word in ["responsible", "accountability", "liable"]):
            return "accountability"
        
        if any(word in text_lower for word in ["contact", "questions", "concerns"]):
            return "challenging_compliance"

        return "openness"  # é»˜è®¤ç±»åˆ«

    def assess_risk(self, params: Dict[str, Any], category: str) -> float:
        """
        è¯„ä¼°éšç§é£é™©åˆ†æ•° (0-1)
        
        åŸºäºæ–‡çŒ®ä¸­çš„é£é™©å› ç´ :
        - æ•æ„Ÿæ•°æ®ç±»å‹
        - ç¬¬ä¸‰æ–¹å…±äº«æ•°é‡
        - æ•°æ®ä¿ç•™æœŸé™
        - å®‰å…¨æªæ–½çš„å­˜åœ¨
        """
        risk_score = 0.0
        
        # å› ç´ 1: æ•æ„Ÿæ•°æ®ç±»å‹
        sensitive_data = {
            "location", "financial", "health", "biometric", 
            "social_security", "password", "credit_card"
        }
        data_types_str = " ".join(params["data_types"]).lower()
        if any(sensitive in data_types_str for sensitive in sensitive_data):
            risk_score += 0.3
        
        # å› ç´ 2: ç¬¬ä¸‰æ–¹å…±äº«
        num_third_parties = len(params["third_parties"])
        if num_third_parties > 0:
            risk_score += min(0.3, num_third_parties * 0.1)
        
        # å› ç´ 3: æ•°æ®ä¿ç•™æœŸé™
        retention = params.get("retention_period", "")
        if retention:
            if "indefinite" in retention.lower() or "forever" in retention.lower():
                risk_score += 0.2
        else:
            risk_score += 0.1  # æœªæ˜ç¡®è¯´æ˜ä¹Ÿæ˜¯é£é™©
        
        # å› ç´ 4: å®‰å…¨æªæ–½ï¼ˆå‡å°‘é£é™©ï¼‰
        if len(params["security_measures"]) > 0:
            risk_score -= 0.1
        
        # å› ç´ 5: ç”¨æˆ·æƒåˆ©ï¼ˆå‡å°‘é£é™©ï¼‰
        if len(params["user_rights"]) >= 3:
            risk_score -= 0.1
        
        return max(0.0, min(1.0, risk_score))
    
    def generate_explanation(self, params: Dict[str, Any], category: str, risk_score: float) -> str:
        """
        Generate explainable analysis description
        
        Args:
            params: Privacy parameters
            category: PIPEDA category
            risk_score: Risk score
            
        Returns:
            Explanation text
        """
        explanation_parts = []
        
        # Category description
        explanation_parts.append(f"This clause falls under the PIPEDA category of '{category}'.")
        
        # Data collection
        if params["data_types"]:
            data_list = ", ".join(params["data_types"][:5])  # Show up to 5
            explanation_parts.append(f"Data types collected include: {data_list}.")
        
        # Data purposes
        if params["purposes"]:
            purpose_list = ", ".join(params["purposes"][:3])
            explanation_parts.append(f"Data usage purposes: {purpose_list}.")
        
        # Third party sharing
        if params["third_parties"]:
            party_count = len(params["third_parties"])
            if party_count > 0:
                explanation_parts.append(f"Data may be shared with {party_count} third parties.")
        
        # Data retention
        if params["retention_period"]:
            explanation_parts.append(f"Data retention period: {params['retention_period']}.")
        
        # User rights
        if params["user_rights"]:
            rights_list = ", ".join(params["user_rights"])
            explanation_parts.append(f"User rights mentioned: {rights_list}.")
        
        # Security measures
        if params["security_measures"]:
            security_list = ", ".join(params["security_measures"][:3])
            explanation_parts.append(f"Security measures: {security_list}.")
        
        # Risk assessment
        risk_level = "Low" if risk_score < 0.3 else "Medium" if risk_score < 0.6 else "High"
        explanation_parts.append(f"\nRisk Assessment: {risk_level} risk (score: {risk_score:.2f})")
        
        if risk_score > 0.5:
            explanation_parts.append("âš ï¸ Recommendation: This clause presents higher privacy risks and requires careful review.")
        
        return "\n".join(explanation_parts)
    
    def analyze_segment(self, text: str) -> Dict[str, Any]:
        """
        åˆ†æå•ä¸ªæ–‡æœ¬æ®µè½
        
        å‚æ•°:
            text: æ–‡æœ¬æ®µè½
            
        è¿”å›:
            åˆ†æç»“æœå­—å…¸
        """
        # å¤„ç†æ–‡æœ¬
        doc = self.nlp(text)
        
        # æå–å‚æ•°
        params = self.extract_privacy_parameters(doc)

        # åˆ†ç±»
        category = self.classify_category(text, params)
        
        # é£é™©è¯„ä¼°
        risk_score = self.assess_risk(params, category)
        
        # ç”Ÿæˆè§£é‡Š
        explanation = self.generate_explanation(params, category, risk_score)
        
        return {
            "text": text,
            "category": category,
            "category_cn": self.PIPEDA_CATEGORIES.get(category, category),
            "parameters": params,
            "risk_score": risk_score,
            "explanation": explanation
        }
    
    def analyze(self, policy_text: str) -> Dict[str, Any]:
        """
        åˆ†æå®Œæ•´çš„éšç§æ”¿ç­–
        
        å‚æ•°:
            policy_text: å®Œæ•´çš„éšç§æ”¿ç­–æ–‡æœ¬
            
        è¿”å›:
            å®Œæ•´çš„åˆ†ææŠ¥å‘Š
        """
        # åˆ†æ®µ
        segments = self.segment_policy(policy_text)
        
        # åˆ†ææ¯ä¸ªæ®µè½
        segment_results = []
        for segment in segments:
            if len(segment.strip()) > 20:  # å¿½ç•¥å¤ªçŸ­çš„æ®µè½
                result = self.analyze_segment(segment)
                segment_results.append(result)
        
        # ç”Ÿæˆæ€»ä½“ç»Ÿè®¡
        total_risk = sum(r["risk_score"] for r in segment_results)
        avg_risk = total_risk / len(segment_results) if segment_results else 0
        
        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        category_counts = {}
        for result in segment_results:
            cat = result["category"]
            category_counts[cat] = category_counts.get(cat, 0) + 1
        
        # æ±‡æ€»æ‰€æœ‰æ•°æ®ç±»å‹å’Œç¬¬ä¸‰æ–¹
        all_data_types = set()
        all_third_parties = set()
        all_purposes = set()
        
        # æ”¶é›†æ•°æ®æ”¶é›†ä¸æ´»åŠ¨çš„å…³è”å…³ç³»
        # æ ¼å¼: {activity/purpose: {data_types: [...], description: "...", segments: [...]}}
        data_collection_by_activity = {}
        
        for result in segment_results:
            params = result["parameters"]
            all_data_types.update(params["data_types"])
            all_third_parties.update(params["third_parties"])
            all_purposes.update(params["purposes"])
            
            # æ„å»ºæ´»åŠ¨æè¿°
            activities = []
            
            # ä¼˜å…ˆä½¿ç”¨è¯¦ç»†çš„purposes
            if params["purposes"]:
                for purpose in params["purposes"]:
                    # å¦‚æœpurposeæ˜¯è¯¦ç»†çŸ­è¯­ï¼ˆé•¿åº¦>5ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                    if len(purpose) > 5:
                        activities.append(purpose)
                    else:
                        # çŸ­è¯ï¼Œå°è¯•æ„å»ºæ›´è¯¦ç»†çš„æè¿°
                        category = result["category"]
                        category_desc = self.PIPEDA_CATEGORIES.get(category, category).replace("_", " ").title()
                        detailed_activity = f"{category_desc}: {purpose}"
                        activities.append(detailed_activity)
            
            # å¦‚æœæ²¡æœ‰purposesä½†æœ‰æ•°æ®æ”¶é›†ï¼Œä½¿ç”¨categoryä½œä¸ºæ´»åŠ¨
            if not activities and params["data_types"]:
                category = result["category"]
                category_desc = self.PIPEDA_CATEGORIES.get(category, category).replace("_", " ").title()
                # å°è¯•ä»æ–‡æœ¬ä¸­æå–ä¸Šä¸‹æ–‡
                text_snippet = result["text"][:150].lower()
                if "when" in text_snippet or "while" in text_snippet or "during" in text_snippet:
                    # æå–æ´»åŠ¨ä¸Šä¸‹æ–‡
                    import re
                    # æ”¹è¿›çš„æ­£åˆ™ï¼šåŒ¹é…æœ‰æ„ä¹‰çš„è¯ï¼Œæ’é™¤æ ‡ç‚¹ç¬¦å·å¼€å¤´
                    activity_match = re.search(r"(?:when|while|during)\s+([a-zA-Z][^.,;:!?]{3,40})", text_snippet)
                    if activity_match:
                        activity_context = activity_match.group(1).strip()
                        # éªŒè¯æå–çš„å†…å®¹æ˜¯å¦æœ‰æ„ä¹‰ï¼ˆè‡³å°‘åŒ…å«2ä¸ªå•è¯ï¼‰
                        if len(activity_context.split()) >= 2:
                            activities.append(f"{category_desc}: {activity_context}")
                        else:
                            activities.append(category_desc)
                    else:
                        activities.append(category_desc)
                else:
                    activities.append(category_desc)
            
            # ä¸ºæ¯ä¸ªæ´»åŠ¨å»ºç«‹æ•°æ®æ”¶é›†å…³è”
            for activity in activities:
                # è¿‡æ»¤æ— æ•ˆçš„æ´»åŠ¨åï¼ˆä»¥æ ‡ç‚¹å¼€å¤´ã€å¤ªçŸ­ç­‰ï¼‰
                activity_clean = activity.strip()
                if (len(activity_clean) < 3 or
                    activity_clean.startswith((',', ';', ':', 'and ', 'or ', 'but ')) or
                    self.is_noise_content(activity_clean)):
                    continue

                if activity_clean not in data_collection_by_activity:
                    data_collection_by_activity[activity_clean] = {
                        "data_types": set(),
                        "description": activity_clean,
                        "segments": []
                    }
                data_collection_by_activity[activity_clean]["data_types"].update(params["data_types"])
                data_collection_by_activity[activity_clean]["segments"].append({
                    "segment_id": len(data_collection_by_activity[activity_clean]["segments"]) + 1,
                    "text_preview": result["text"][:150] + "..." if len(result["text"]) > 150 else result["text"],
                    "risk_score": result["risk_score"]
                })
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        data_collection_summary = {}
        for activity, info in data_collection_by_activity.items():
            data_collection_summary[activity] = {
                "data_types": sorted(list(info["data_types"])),
                "description": info["description"],
                "segment_count": len(info["segments"]),
                "segments": info["segments"][:3]  # åªä¿ç•™å‰3ä¸ªæ®µè½ç¤ºä¾‹
            }
        
        return {
            "summary": {
                "total_segments": len(segment_results),
                "average_risk_score": round(avg_risk, 2),
                "category_distribution": category_counts,
                "total_data_types": list(all_data_types),
                "total_third_parties": list(all_third_parties),
                "total_purposes": list(all_purposes),
                "data_collection_by_activity": data_collection_summary
            },
            "segment_analyses": segment_results
        }
    
    def generate_report(self, analysis_results: Dict[str, Any], output_format="markdown") -> str:
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        
        å‚æ•°:
            analysis_results: analyze()çš„è¿”å›ç»“æœ
            output_format: è¾“å‡ºæ ¼å¼ ("markdown" æˆ– "text")
            
        è¿”å›:
            æ ¼å¼åŒ–çš„æŠ¥å‘Šæ–‡æœ¬
        """
        if output_format == "markdown":
            return self._generate_markdown_report(analysis_results)
        else:
            return self._generate_text_report(analysis_results)
    
    def _generate_markdown_report(self, results: Dict[str, Any]) -> str:
        """Generate Markdown format report"""
        summary = results["summary"]
        segments = results["segment_analyses"]
        
        report = []
        report.append("# Privacy Policy Analysis Report\n")
        
        # Summary
        report.append("## Summary\n")
        report.append(f"- **Segments Analyzed**: {summary['total_segments']}")
        report.append(f"- **Average Risk Score**: {summary['average_risk_score']:.2f}")
        report.append(f"- **Data Types Found**: {len(summary['total_data_types'])} types")
        report.append(f"- **Third Parties Involved**: {len(summary['total_third_parties'])} entities\n")
        
        # Category distribution
        report.append("## PIPEDA Category Distribution\n")
        for category, count in sorted(summary['category_distribution'].items(), 
                                      key=lambda x: x[1], reverse=True):
            report.append(f"- {category}: {count} segments")
        report.append("\n")
        
        # Data types collected
        if summary['total_data_types']:
            report.append("## Data Types Collected\n")
            for dt in sorted(summary['total_data_types'])[:20]:  # Show top 20
                report.append(f"- {dt}")
            report.append("\n")
        
        # Third parties involved
        if summary['total_third_parties']:
            report.append("## Third Parties Involved\n")
            for tp in sorted(summary['total_third_parties'])[:20]:
                report.append(f"- {tp}")
            report.append("\n")
        
        # Data collection by activity/purpose
        if summary.get('data_collection_by_activity'):
            report.append("## ğŸ“Š Data Collection by Activity\n")
            report.append("_What data is collected on what activities, based on their privacy policy_\n\n")
            
            data_collection = summary['data_collection_by_activity']
            # Sort by activity name for consistent output
            for activity in sorted(data_collection.keys()):
                activity_info = data_collection[activity]
                data_types = activity_info.get('data_types', [])
                description = activity_info.get('description', activity)
                segment_count = activity_info.get('segment_count', 0)
                
                if data_types:
                    report.append(f"### {description}\n")
                    report.append(f"**Activity Context**: {description}\n")
                    report.append(f"**Segments Found**: {segment_count}\n")
                    report.append("**Data Collected:**\n")
                    # Show all data types, but limit display if too many
                    display_types = data_types[:15]  # Show up to 15 types
                    for dt in display_types:
                        report.append(f"- {dt}")
                    if len(data_types) > 15:
                        report.append(f"- ... and {len(data_types) - 15} more")
                    
                    # Show example segments if available
                    if activity_info.get('segments'):
                        report.append("\n**Example Segments:**\n")
                        for seg in activity_info['segments'][:2]:  # Show first 2 examples
                            report.append(f"- *Risk: {seg.get('risk_score', 0):.2f}* - {seg.get('text_preview', '')}\n")
                    report.append("\n")
            report.append("\n")
        
        # High risk segments
        high_risk_segments = [s for s in segments if s['risk_score'] > 0.5]
        if high_risk_segments:
            report.append("## âš ï¸ High Risk Segments\n")
            for i, segment in enumerate(high_risk_segments[:5], 1):  # Show top 5
                report.append(f"### Segment {i} (Risk Score: {segment['risk_score']:.2f})\n")
                report.append(f"**Text**: {segment['text'][:200]}...\n")
                report.append(f"**Analysis**:\n{segment['explanation']}\n")
        
        # Detailed analysis
        report.append("## Detailed Analysis\n")
        for i, segment in enumerate(segments, 1):
            report.append(f"### Segment {i}\n")
            report.append(f"**Text**: {segment['text']}\n")
            report.append(f"**Category**: {segment['category']}\n")
            report.append(f"**Risk Score**: {segment['risk_score']:.2f}\n")
            report.append(f"**Analysis**:\n{segment['explanation']}\n")
            report.append("---\n")
        
        return "\n".join(report)
    
    def _generate_text_report(self, results: Dict[str, Any]) -> str:
        """Generate plain text format report"""
        summary = results["summary"]
        segments = results["segment_analyses"]
        
        report = []
        report.append("=" * 60)
        report.append("Privacy Policy Analysis Report")
        report.append("=" * 60)
        report.append("")
        
        report.append("Summary:")
        report.append(f"  Segments analyzed: {summary['total_segments']}")
        report.append(f"  Average risk score: {summary['average_risk_score']:.2f}")
        report.append(f"  Data types found: {len(summary['total_data_types'])}")
        report.append(f"  Third parties: {len(summary['total_third_parties'])}")
        report.append("")
        
        # Detailed analysis
        for i, segment in enumerate(segments, 1):
            report.append("-" * 60)
            report.append(f"Segment {i}:")
            report.append(f"Category: {segment['category']}")
            report.append(f"Risk score: {segment['risk_score']:.2f}")
            report.append(f"\n{segment['explanation']}")
            report.append("")
        
        return "\n".join(report)


def main():
    """
    ç¤ºä¾‹ç”¨æ³•
    """
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = PrivacyPolicyAnalyzer()
    
    # ç¤ºä¾‹éšç§æ”¿ç­–æ–‡æœ¬
    sample_policy = """
    Information We Collect
    
    We collect personal information that you provide to us, including your name, 
    email address, phone number, and location data. This information is used to 
    provide and improve our services.
    
    How We Share Your Information
    
    We may share your personal data with third-party service providers, advertising 
    partners, and analytics companies to help us operate our business. We also share 
    information with law enforcement when required by law.
    
    Your Rights
    
    You have the right to access, correct, or delete your personal information. 
    You may also withdraw your consent at any time by contacting us.
    
    Data Security
    
    We implement appropriate technical and organizational measures to protect your 
    personal data, including encryption and secure servers.
    """
    
    # æ‰§è¡Œåˆ†æ
    print("æ­£åœ¨åˆ†æéšç§æ”¿ç­–...\n")
    results = analyzer.analyze(sample_policy)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = analyzer.generate_report(results, output_format="text")
    print(report)
    
    # ä¹Ÿå¯ä»¥ç”ŸæˆMarkdownæŠ¥å‘Š
    # markdown_report = analyzer.generate_report(results, output_format="markdown")
    # with open("privacy_analysis_report.md", "w", encoding="utf-8") as f:
    #     f.write(markdown_report)


if __name__ == "__main__":
    main()






