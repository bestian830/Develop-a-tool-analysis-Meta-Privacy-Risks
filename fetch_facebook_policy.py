"""
Facebookéšç§æ”¿ç­–æŠ“å–å·¥å…·

ç”±äºFacebookæœ‰åçˆ¬è™«ä¿æŠ¤ï¼Œè¿™ä¸ªè„šæœ¬æä¾›å‡ ç§æ–¹æ³•ï¼š
1. ä½¿ç”¨Seleniumæ¨¡æ‹Ÿæµè§ˆå™¨ï¼ˆéœ€è¦å®‰è£…Chrome/Firefoxï¼‰
2. ä½¿ç”¨requests + headersä¼ªè£…
3. æ‰‹åŠ¨è¾“å…¥ï¼ˆæœ€å¯é ï¼‰
"""

import requests
from pathlib import Path


def fetch_with_requests(url: str) -> str:
    """
    æ–¹æ³•1: ä½¿ç”¨requestsåº“ï¼ˆå¯èƒ½è¢«é˜»æ­¢ï¼‰
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }

    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"âŒ Requestsæ–¹æ³•å¤±è´¥: {e}")
        return None


def fetch_with_selenium(url: str) -> str:
    """
    æ–¹æ³•2: ä½¿ç”¨Seleniumï¼ˆéœ€è¦å®‰è£…: pip install seleniumï¼‰
    """
    try:
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        import time

        # é…ç½®Chromeé€‰é¡¹
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')

        print("ğŸŒ å¯åŠ¨æµè§ˆå™¨...")
        driver = webdriver.Chrome(options=chrome_options)

        print(f"ğŸ“„ è®¿é—®é¡µé¢: {url}")
        driver.get(url)

        # ç­‰å¾…é¡µé¢åŠ è½½
        time.sleep(5)

        # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
        try:
            # Facebookéšç§æ”¿ç­–é€šå¸¸åœ¨ç‰¹å®šçš„divä¸­
            content = driver.find_element(By.TAG_NAME, "body").text
        except:
            content = driver.page_source

        driver.quit()
        return content

    except ImportError:
        print("âŒ Seleniumæœªå®‰è£…ã€‚è¿è¡Œ: pip install selenium")
        print("   è¿˜éœ€è¦ä¸‹è½½ChromeDriver: https://chromedriver.chromium.org/")
        return None
    except Exception as e:
        print(f"âŒ Seleniumæ–¹æ³•å¤±è´¥: {e}")
        return None


def manual_input_guide():
    """
    æ–¹æ³•3: æ‰‹åŠ¨è¾“å…¥æŒ‡å—
    """
    print("\n" + "="*80)
    print("ğŸ“ æ‰‹åŠ¨è·å–Facebookéšç§æ”¿ç­–æŒ‡å—")
    print("="*80)
    print("""
1. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ä»¥ä¸‹é“¾æ¥ï¼š
   https://www.facebook.com/privacy/policy
   æˆ–
   https://www.facebook.com/privacy/policy/?entry_point=data_policy_redirect

2. ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½

3. æ–¹å¼A - å¤åˆ¶æ•´ä¸ªé¡µé¢ï¼š
   - æŒ‰ Ctrl+A (Windows) æˆ– Cmd+A (Mac) å…¨é€‰
   - æŒ‰ Ctrl+C (Windows) æˆ– Cmd+C (Mac) å¤åˆ¶
   - åˆ›å»ºæ–°æ–‡ä»¶ facebook_policy_v1.txt å¹¶ç²˜è´´

4. æ–¹å¼B - æµè§ˆå™¨ä¿å­˜ï¼š
   - æŒ‰ Ctrl+S (Windows) æˆ– Cmd+S (Mac)
   - é€‰æ‹©ä¿å­˜æ ¼å¼ä¸º "Text" æˆ– "txt"
   - ä¿å­˜ä¸º facebook_policy_v1.txt

5. æ–¹å¼C - ä½¿ç”¨æ‰“å°åŠŸèƒ½ï¼š
   - æŒ‰ Ctrl+P (Windows) æˆ– Cmd+P (Mac)
   - é€‰æ‹© "Save as PDF" æˆ– "Print to PDF"
   - ä¹‹åå¯ä»¥ç”¨PDFè½¬æ–‡æœ¬å·¥å…·è½¬æ¢

6. éªŒè¯æ–‡ä»¶ï¼š
   - ç¡®ä¿æ–‡ä»¶è‡³å°‘æœ‰å‡ åƒå­—
   - åŒ…å«å®Œæ•´çš„ç« èŠ‚æ ‡é¢˜
   - å¼€å¤´åº”è¯¥æ˜¯ "Meta Privacy Policy" æˆ–ç±»ä¼¼æ ‡é¢˜
""")
    print("="*80)


def clean_html_text(html_content: str) -> str:
    """
    æ¸…ç†HTMLå†…å®¹ï¼Œæå–çº¯æ–‡æœ¬
    """
    try:
        from bs4 import BeautifulSoup

        soup = BeautifulSoup(html_content, 'html.parser')

        # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
        for script in soup(["script", "style", "nav", "header", "footer"]):
            script.decompose()

        # è·å–æ–‡æœ¬
        text = soup.get_text()

        # æ¸…ç†ç©ºè¡Œ
        lines = [line.strip() for line in text.splitlines()]
        lines = [line for line in lines if line]

        return '\n\n'.join(lines)
    except ImportError:
        print("âš ï¸  BeautifulSoupæœªå®‰è£…ã€‚è¿è¡Œ: pip install beautifulsoup4")
        return html_content


def main():
    print("="*80)
    print("Facebookéšç§æ”¿ç­–æŠ“å–å·¥å…·")
    print("="*80)

    url = input("\nè¯·è¾“å…¥Facebookéšç§æ”¿ç­–URLï¼ˆç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤ï¼‰: ").strip()
    if not url:
        url = "https://www.facebook.com/privacy/policy"

    print(f"\nç›®æ ‡URL: {url}")
    print("\nå°è¯•è·å–å†…å®¹...\n")

    content = None

    # å°è¯•æ–¹æ³•1: Requests
    print("å°è¯•æ–¹æ³•1: HTTPè¯·æ±‚...")
    content = fetch_with_requests(url)

    if content and len(content) > 1000:
        print("âœ… æˆåŠŸè·å–å†…å®¹ï¼")
        content = clean_html_text(content)
    else:
        # å°è¯•æ–¹æ³•2: Selenium
        print("\nå°è¯•æ–¹æ³•2: æµè§ˆå™¨æ¨¡æ‹Ÿ...")
        content = fetch_with_selenium(url)

    if content and len(content) > 1000:
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼Œé¿å…è¦†ç›–
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä»URLä¸­æå–ç½‘ç«™åç§°
        import re
        domain_match = re.search(r'://(?:www\.)?([^/\.]+)', url)
        site_name = domain_match.group(1) if domain_match else "policy"

        # ç”Ÿæˆæ–‡ä»¶åï¼š[ç½‘ç«™å]_policy_[æ—¶é—´æˆ³].txt
        output_file = f"{site_name}_policy_{timestamp}.txt"

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™æ·»åŠ åºå·
        counter = 1
        original_output = output_file
        while Path(output_file).exists():
            output_file = f"{site_name}_policy_{timestamp}_{counter}.txt"
            counter += 1

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"\nâœ… æˆåŠŸä¿å­˜åˆ°: {output_file}")
        print(f"   æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
        print(f"   æ–‡ä»¶è·¯å¾„: {Path(output_file).absolute()}")

        # æ˜¾ç¤ºå‰500å­—ç¬¦é¢„è§ˆ
        print("\nğŸ“„ å†…å®¹é¢„è§ˆï¼ˆå‰500å­—ç¬¦ï¼‰:")
        print("-" * 80)
        print(content[:500])
        print("-" * 80)
    else:
        # å¦‚æœè‡ªåŠ¨æ–¹æ³•éƒ½å¤±è´¥ï¼Œæ˜¾ç¤ºæ‰‹åŠ¨æŒ‡å—
        print("\nâš ï¸  è‡ªåŠ¨è·å–å¤±è´¥ã€‚Facebookæœ‰åçˆ¬è™«ä¿æŠ¤ã€‚")
        manual_input_guide()

        # è¯¢é—®æ˜¯å¦æ‰‹åŠ¨è¾“å…¥
        print("\næ˜¯å¦è¦ç°åœ¨æ‰‹åŠ¨ç²˜è´´å†…å®¹ï¼Ÿ(y/n): ", end='')
        choice = input().strip().lower()

        if choice == 'y':
            print("\nè¯·ç²˜è´´Facebookéšç§æ”¿ç­–çš„å®Œæ•´å†…å®¹ï¼ˆç²˜è´´å®ŒæˆåæŒ‰Ctrl+Dï¼ˆMac/Linuxï¼‰æˆ–Ctrl+Z+Enterï¼ˆWindowsï¼‰ç»“æŸï¼‰:")
            print("-" * 80)

            lines = []
            try:
                while True:
                    line = input()
                    lines.append(line)
            except EOFError:
                pass

            content = '\n'.join(lines)

            if len(content) > 100:
                # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
                from datetime import datetime
                import re
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

                # ä»URLä¸­æå–ç½‘ç«™åç§°
                domain_match = re.search(r'://(?:www\.)?([^/\.]+)', url)
                site_name = domain_match.group(1) if domain_match else "policy"

                output_file = f"{site_name}_policy_{timestamp}.txt"

                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                counter = 1
                while Path(output_file).exists():
                    output_file = f"{site_name}_policy_{timestamp}_{counter}.txt"
                    counter += 1

                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(content)
                print(f"\nâœ… å·²ä¿å­˜åˆ°: {output_file}")
                print(f"   æ–‡ä»¶è·¯å¾„: {Path(output_file).absolute()}")
            else:
                print("\nâŒ å†…å®¹å¤ªçŸ­ï¼Œæœªä¿å­˜")


if __name__ == "__main__":
    main()
