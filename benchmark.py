"""
åŸºå‡†æµ‹è¯•å·¥å…· - æ¯”è¾ƒäººå·¥æ ‡æ³¨ä¸å·¥å…·åˆ†æç»“æœ

è¿™ä¸ªè„šæœ¬ç”¨äºè¯„ä¼°éšç§æ”¿ç­–åˆ†æå™¨çš„æ€§èƒ½ï¼Œé€šè¿‡æ¯”è¾ƒï¼š
1. äººå·¥æ ‡æ³¨ç»“æœ
2. å·¥å…·è‡ªåŠ¨åˆ†æç»“æœ

è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬ï¼š
- ç±»åˆ«ä¸€è‡´æ€§ï¼ˆCohen's Kappaï¼‰
- å‚æ•°æå–çš„ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
- é£é™©è¯„åˆ†çš„ç›¸å…³æ€§
"""

import json
from typing import Dict, List, Tuple
import statistics
from pathlib import Path
from privacy_analyzer_example import PrivacyPolicyAnalyzer


class BenchmarkEvaluator:
    """åŸºå‡†æµ‹è¯•è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.analyzer = PrivacyPolicyAnalyzer()
    
    def calculate_set_metrics(self, human_set: set, tool_set: set) -> Dict[str, float]:
        """
        è®¡ç®—é›†åˆç±»æŒ‡æ ‡ï¼ˆç”¨äºæ•°æ®ç±»å‹ã€ç¬¬ä¸‰æ–¹ç­‰ï¼‰
        
        è¿”å›ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
        """
        if len(tool_set) == 0:
            precision = 0.0
        else:
            intersection = len(human_set & tool_set)
            precision = intersection / len(tool_set)
        
        if len(human_set) == 0:
            recall = 1.0 if len(tool_set) == 0 else 0.0
        else:
            intersection = len(human_set & tool_set)
            recall = intersection / len(human_set)
        
        if precision + recall == 0:
            f1 = 0.0
        else:
            f1 = 2 * (precision * recall) / (precision + recall)
        
        return {
            "precision": precision,
            "recall": recall,
            "f1": f1
        }
    
    def evaluate_segment(self, human_annotation: Dict, segment_text: str) -> Dict:
        """
        è¯„ä¼°å•ä¸ªæ®µè½çš„åˆ†æç»“æœ
        
        å‚æ•°:
            human_annotation: äººå·¥æ ‡æ³¨ç»“æœ
            segment_text: æ®µè½æ–‡æœ¬
            
        è¿”å›:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        # ä½¿ç”¨å·¥å…·åˆ†æ
        tool_result = self.analyzer.analyze_segment(segment_text)
        
        # 1. ç±»åˆ«ä¸€è‡´æ€§
        category_match = (human_annotation.get("category") == tool_result["category"])
        
        # 2. æ•°æ®ç±»å‹åŒ¹é…
        human_data_types = set(human_annotation.get("data_types", []))
        tool_data_types = set(tool_result["parameters"]["data_types"])
        data_type_metrics = self.calculate_set_metrics(human_data_types, tool_data_types)
        
        # 3. ç¬¬ä¸‰æ–¹åŒ¹é…
        human_third_parties = set(human_annotation.get("third_parties", []))
        tool_third_parties = set(tool_result["parameters"]["third_parties"])
        third_party_metrics = self.calculate_set_metrics(human_third_parties, tool_third_parties)
        
        # 4. ç”¨æˆ·æƒåˆ©åŒ¹é…
        human_rights = set(human_annotation.get("user_rights", []))
        tool_rights = set(tool_result["parameters"]["user_rights"])
        rights_metrics = self.calculate_set_metrics(human_rights, tool_rights)
        
        # 5. é£é™©åˆ†æ•°å·®å¼‚
        human_risk = human_annotation.get("risk_score", 0.5)
        tool_risk = tool_result["risk_score"]
        risk_difference = abs(human_risk - tool_risk)
        
        return {
            "segment_text": segment_text[:100] + "..." if len(segment_text) > 100 else segment_text,
            "category_match": category_match,
            "human_category": human_annotation.get("category"),
            "tool_category": tool_result["category"],
            "data_type_metrics": data_type_metrics,
            "third_party_metrics": third_party_metrics,
            "rights_metrics": rights_metrics,
            "risk_difference": risk_difference,
            "human_risk": human_risk,
            "tool_risk": tool_risk,
            "tool_result": tool_result
        }
    
    def evaluate_dataset(self, annotations_file: str) -> Dict:
        """
        è¯„ä¼°æ•´ä¸ªæ•°æ®é›†
        
        å‚æ•°:
            annotations_file: äººå·¥æ ‡æ³¨æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰
            
        è¿”å›:
            æ€»ä½“è¯„ä¼°ç»“æœ
        """
        # åŠ è½½äººå·¥æ ‡æ³¨
        with open(annotations_file, "r", encoding="utf-8") as f:
            annotations = json.load(f)
        
        segment_results = []
        
        for annotation in annotations:
            segment_text = annotation["text"]
            result = self.evaluate_segment(annotation, segment_text)
            segment_results.append(result)
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_segments = len(segment_results)
        
        # ç±»åˆ«ä¸€è‡´æ€§
        category_matches = sum(1 for r in segment_results if r["category_match"])
        category_accuracy = category_matches / total_segments if total_segments > 0 else 0
        
        # æ•°æ®ç±»å‹çš„å¹³å‡æŒ‡æ ‡
        avg_data_type_precision = statistics.mean([r["data_type_metrics"]["precision"] for r in segment_results])
        avg_data_type_recall = statistics.mean([r["data_type_metrics"]["recall"] for r in segment_results])
        avg_data_type_f1 = statistics.mean([r["data_type_metrics"]["f1"] for r in segment_results])
        
        # ç¬¬ä¸‰æ–¹çš„å¹³å‡æŒ‡æ ‡
        avg_third_party_precision = statistics.mean([r["third_party_metrics"]["precision"] for r in segment_results])
        avg_third_party_recall = statistics.mean([r["third_party_metrics"]["recall"] for r in segment_results])
        avg_third_party_f1 = statistics.mean([r["third_party_metrics"]["f1"] for r in segment_results])
        
        # é£é™©è¯„åˆ†ç›¸å…³æ€§
        avg_risk_difference = statistics.mean([r["risk_difference"] for r in segment_results])
        
        # è®¡ç®—Pearsonç›¸å…³ç³»æ•°ï¼ˆé£é™©åˆ†æ•°ï¼‰
        human_risks = [r["human_risk"] for r in segment_results]
        tool_risks = [r["tool_risk"] for r in segment_results]
        
        if len(human_risks) > 1:
            # ç®€å•çš„Pearsonç›¸å…³ç³»æ•°è®¡ç®—
            mean_human = statistics.mean(human_risks)
            mean_tool = statistics.mean(tool_risks)
            
            numerator = sum((h - mean_human) * (t - mean_tool) for h, t in zip(human_risks, tool_risks))
            
            denom_human = sum((h - mean_human) ** 2 for h in human_risks)
            denom_tool = sum((t - mean_tool) ** 2 for t in tool_risks)
            
            if denom_human > 0 and denom_tool > 0:
                correlation = numerator / ((denom_human ** 0.5) * (denom_tool ** 0.5))
            else:
                correlation = 0.0
        else:
            correlation = 0.0
        
        return {
            "total_segments": total_segments,
            "category_accuracy": category_accuracy,
            "data_type_metrics": {
                "precision": avg_data_type_precision,
                "recall": avg_data_type_recall,
                "f1": avg_data_type_f1
            },
            "third_party_metrics": {
                "precision": avg_third_party_precision,
                "recall": avg_third_party_recall,
                "f1": avg_third_party_f1
            },
            "risk_correlation": correlation,
            "avg_risk_difference": avg_risk_difference,
            "segment_results": segment_results
        }
    
    def generate_report(self, evaluation_results: Dict, output_file: str = None):
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        å‚æ•°:
            evaluation_results: evaluate_datasetçš„è¿”å›ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        report_lines = []
        
        report_lines.append("=" * 70)
        report_lines.append("éšç§æ”¿ç­–åˆ†æå™¨ - åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        report_lines.append("=" * 70)
        report_lines.append("")
        
        report_lines.append("## æ€»ä½“è¯„ä¼°æŒ‡æ ‡\n")
        report_lines.append(f"æµ‹è¯•æ®µè½æ•°: {evaluation_results['total_segments']}")
        report_lines.append("")
        
        report_lines.append("### ç±»åˆ«åˆ†ç±»å‡†ç¡®æ€§")
        report_lines.append(f"å‡†ç¡®ç‡: {evaluation_results['category_accuracy']:.2%}")
        report_lines.append("")
        
        report_lines.append("### æ•°æ®ç±»å‹æå–")
        dt = evaluation_results['data_type_metrics']
        report_lines.append(f"ç²¾ç¡®ç‡ (Precision): {dt['precision']:.2%}")
        report_lines.append(f"å¬å›ç‡ (Recall):    {dt['recall']:.2%}")
        report_lines.append(f"F1 åˆ†æ•°:            {dt['f1']:.2%}")
        report_lines.append("")
        
        report_lines.append("### ç¬¬ä¸‰æ–¹è¯†åˆ«")
        tp = evaluation_results['third_party_metrics']
        report_lines.append(f"ç²¾ç¡®ç‡ (Precision): {tp['precision']:.2%}")
        report_lines.append(f"å¬å›ç‡ (Recall):    {tp['recall']:.2%}")
        report_lines.append(f"F1 åˆ†æ•°:            {tp['f1']:.2%}")
        report_lines.append("")
        
        report_lines.append("### é£é™©è¯„åˆ†")
        report_lines.append(f"ä¸äººå·¥æ ‡æ³¨çš„ç›¸å…³æ€§: {evaluation_results['risk_correlation']:.3f}")
        report_lines.append(f"å¹³å‡åˆ†æ•°å·®å¼‚:       {evaluation_results['avg_risk_difference']:.3f}")
        report_lines.append("")
        
        report_lines.append("=" * 70)
        report_lines.append("## è¯¦ç»†ç»“æœ\n")
        
        # æŒ‰ç±»åˆ«ä¸€è‡´æ€§æ’åºï¼Œæ˜¾ç¤ºä¸ä¸€è‡´çš„æ¡ˆä¾‹
        mismatched = [r for r in evaluation_results['segment_results'] if not r['category_match']]
        
        if mismatched:
            report_lines.append(f"### ç±»åˆ«ä¸ä¸€è‡´çš„æ®µè½ ({len(mismatched)} ä¸ª):\n")
            for i, result in enumerate(mismatched[:10], 1):  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                report_lines.append(f"æ®µè½ {i}:")
                report_lines.append(f"  æ–‡æœ¬: {result['segment_text']}")
                report_lines.append(f"  äººå·¥æ ‡æ³¨: {result['human_category']}")
                report_lines.append(f"  å·¥å…·åˆ†æ: {result['tool_category']}")
                report_lines.append("")
        
        # é«˜é£é™©åˆ†æ•°å·®å¼‚çš„æ¡ˆä¾‹
        high_diff = sorted(evaluation_results['segment_results'], 
                          key=lambda x: x['risk_difference'], reverse=True)[:5]
        
        if high_diff:
            report_lines.append("### é£é™©è¯„åˆ†å·®å¼‚æœ€å¤§çš„æ®µè½ (å‰5ä¸ª):\n")
            for i, result in enumerate(high_diff, 1):
                report_lines.append(f"æ®µè½ {i}:")
                report_lines.append(f"  æ–‡æœ¬: {result['segment_text']}")
                report_lines.append(f"  äººå·¥è¯„åˆ†: {result['human_risk']:.2f}")
                report_lines.append(f"  å·¥å…·è¯„åˆ†: {result['tool_risk']:.2f}")
                report_lines.append(f"  å·®å¼‚: {result['risk_difference']:.2f}")
                report_lines.append("")
        
        report_lines.append("=" * 70)
        report_lines.append("è¯„ä¼°å®Œæˆ")
        report_lines.append("=" * 70)
        
        report_text = "\n".join(report_lines)
        
        if output_file:
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(report_text)
            print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        
        return report_text


def create_sample_annotations():
    """
    åˆ›å»ºç¤ºä¾‹æ ‡æ³¨æ–‡ä»¶ï¼ˆç”¨äºæ¼”ç¤ºï¼‰
    """
    sample_annotations = [
        {
            "text": "We collect your name, email address, and location data when you use our services.",
            "category": "limiting_collection",
            "data_types": ["name", "email", "address", "location", "data"],
            "third_parties": [],
            "user_rights": [],
            "risk_score": 0.3
        },
        {
            "text": "We share your information with advertising partners and analytics companies.",
            "category": "limiting_use",
            "data_types": ["information"],
            "third_parties": ["partners", "companies"],
            "user_rights": [],
            "risk_score": 0.6
        },
        {
            "text": "You have the right to access, correct, and delete your personal information.",
            "category": "individual_access",
            "data_types": ["information"],
            "third_parties": [],
            "user_rights": ["access", "correct", "delete"],
            "risk_score": 0.2
        },
        {
            "text": "By using our services, you consent to our collection and use of your data.",
            "category": "consent",
            "data_types": ["data"],
            "third_parties": [],
            "user_rights": [],
            "risk_score": 0.4
        },
        {
            "text": "We implement encryption and secure servers to protect your data.",
            "category": "safeguards",
            "data_types": ["data"],
            "third_parties": [],
            "user_rights": [],
            "risk_score": 0.2
        }
    ]
    
    output_file = "sample_annotations.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(sample_annotations, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ ç¤ºä¾‹æ ‡æ³¨æ–‡ä»¶å·²åˆ›å»º: {output_file}")
    return output_file


def main():
    """ä¸»å‡½æ•° - è¿è¡ŒåŸºå‡†æµ‹è¯•"""
    import argparse
    
    parser = argparse.ArgumentParser(description="éšç§æ”¿ç­–åˆ†æå™¨åŸºå‡†æµ‹è¯•å·¥å…·")
    parser.add_argument(
        "annotations_file",
        nargs="?",
        help="äººå·¥æ ‡æ³¨JSONæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸æä¾›ï¼Œå°†åˆ›å»ºç¤ºä¾‹æ–‡ä»¶ï¼‰"
    )
    parser.add_argument(
        "-o", "--output",
        default="benchmark_report.txt",
        help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--create-sample",
        action="store_true",
        help="åˆ›å»ºç¤ºä¾‹æ ‡æ³¨æ–‡ä»¶"
    )
    
    args = parser.parse_args()
    
    # å¦‚æœè¯·æ±‚åˆ›å»ºç¤ºä¾‹æˆ–æ²¡æœ‰æä¾›æ–‡ä»¶
    if args.create_sample or not args.annotations_file:
        annotations_file = create_sample_annotations()
        if args.create_sample:
            print("\næç¤º: è¯·ç¼–è¾‘ sample_annotations.json æ·»åŠ æ‚¨çš„äººå·¥æ ‡æ³¨")
            print("ç„¶åè¿è¡Œ: python benchmark.py sample_annotations.json")
            return
    else:
        annotations_file = args.annotations_file
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(annotations_file).exists():
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {annotations_file}")
        print("æç¤º: ä½¿ç”¨ --create-sample åˆ›å»ºç¤ºä¾‹æ–‡ä»¶")
        return
    
    print("ğŸ”§ åˆå§‹åŒ–åŸºå‡†æµ‹è¯•è¯„ä¼°å™¨...")
    evaluator = BenchmarkEvaluator()
    
    print(f"ğŸ“„ åŠ è½½æ ‡æ³¨æ–‡ä»¶: {annotations_file}")
    print("ğŸ” æ­£åœ¨è¯„ä¼°...")
    
    # è¿è¡Œè¯„ä¼°
    results = evaluator.evaluate_dataset(annotations_file)
    
    print("\n" + "=" * 70)
    print("è¯„ä¼°å®Œæˆï¼ä¸»è¦æŒ‡æ ‡ï¼š")
    print("=" * 70)
    print(f"ç±»åˆ«å‡†ç¡®ç‡:         {results['category_accuracy']:.2%}")
    print(f"æ•°æ®ç±»å‹ F1:        {results['data_type_metrics']['f1']:.2%}")
    print(f"ç¬¬ä¸‰æ–¹è¯†åˆ« F1:      {results['third_party_metrics']['f1']:.2%}")
    print(f"é£é™©è¯„åˆ†ç›¸å…³æ€§:     {results['risk_correlation']:.3f}")
    print("=" * 70)
    
    # ç”ŸæˆæŠ¥å‘Š
    print(f"\nğŸ“ ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
    evaluator.generate_report(results, args.output)
    
    print(f"\nâœ“ å®Œæˆï¼è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")


if __name__ == "__main__":
    main()




