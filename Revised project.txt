Methodology
0. Clarify the exact output you want
I’d recommend you define a claim schema first. For each “fact” in the policy:
{
  "doc_id": "meta_fb_2024-09",
  "section": "Things you and others do and provide",
  "clause_id": 42,
  "activity": ["take_photo_with_glasses", "use_voice_assistant"],
  "data_collected": ["audio", "video", "location", "device_info"],
  "source_text": "When you use the Ray-Ban Meta smart glasses to take photos or videos, we collect audio and visual information and related log data..."
}


Once you have this structure, all your code is about:
1. Splitting the policies into clauses,

2. Tagging each clause with activities + data types (+ maybe purpose),

3. Saving JSON per policy version and then diffing them.

________________


1. Collect and store the policies (for each version)
Even if you only have a few versions, script it so it’s reproducible.
a) Download HTML
Use requests + BeautifulSoup:
import requests
from bs4 import BeautifulSoup
from pathlib import Path


def fetch_policy(url, out_path):
    r = requests.get(url)
    r.raise_for_status()
    Path(out_path).write_text(r.text, encoding="utf-8")


# Example
fetch_policy(
    "https://www.ray-ban.com/uk/c/privacy-policy",
    "data/raw/rayban_privacy_2021-09-27.html"
)


For the Facebook/Meta URLs, you’ll do the same (for each version you care about).
b) Strip HTML → structured text
You don’t want just raw text; you want sections and paragraphs.
from bs4 import BeautifulSoup
import json


def html_to_sections(html_path, out_json):
    html = Path(html_path).read_text(encoding="utf-8")
    soup = BeautifulSoup(html, "html.parser")


    sections = []
    current_heading = "ROOT"
    for elem in soup.find_all(["h1", "h2", "h3", "p", "li"]):
        if elem.name in ["h1", "h2", "h3"]:
            current_heading = elem.get_text(strip=True)
        else:
            text = elem.get_text(" ", strip=True)
            if text:
                sections.append({
                    "section": current_heading,
                    "text": text
                })


    Path(out_json).write_text(json.dumps(sections, indent=2), encoding="utf-8")


# Example
html_to_sections(
    "data/raw/rayban_privacy_2021-09-27.html",
    "data/clean/rayban_privacy_2021-09-27.json"
)


Now you have a list of {section, text} objects for each policy version.
________________


2. Design your ontology: activities & data categories
Do this on paper first. Look through a couple of policies manually and list:
a) Data categories (what data)
Examples (you can expand later):
   * identifier: name, email, phone, username, IP

   * location: GPS, precise location, city, country, IP-based location

   * audio: voice recordings, microphone audio

   * video_image: photos, videos, camera content, images

   * contacts: address book, contacts list

   * communications: messages, calls, chats

   * device_info: device identifiers, OS, browser, hardware, battery level

   * usage_events: app usage, log data, events, clicks

   * payment: credit card, transaction info

   * biometrics: facial geometry, voiceprint, retinal scan

   * health: prescription data, medical info

b) Activities (when/where it’s collected)
Think specifically for Meta + Ray-Ban eyewear:
      * take_photo_with_glasses

      * record_video_with_glasses

      * use_voice_assistant (say “Hey Meta”)

      * stream_media (listen to music)

      * share_to_facebook / post_to_social

      * use_companion_app (View app)

      * visit_rayban_website

      * create_account

      * purchase_glasses

      * contact_support

      * enable_location_services

You do not have to be perfect at first. You can refine as you read more policies.
Represent this in code as dictionaries of keywords.
DATA_LEXICON = {
    "location": ["location", "geolocation", "gps", "locational", "where you are"],
    "audio": ["audio", "voice recording", "microphone"],
    "video_image": ["photo", "photos", "picture", "video", "image", "visual information"],
    "device_info": ["device information", "device info", "hardware", "software version",
                    "browser", "operating system", "device identifiers", "cookie"],
    # ...
}


ACTIVITY_LEXICON = {
    "take_photo_with_glasses": ["when you take photos", "when you take a photo",
                                "when you capture photos", "when you use the camera on the glasses"],
    "record_video_with_glasses": ["when you record video", "when you capture videos"],
    "use_voice_assistant": ["when you say 'hey meta'", "when you use voice commands",
                            "voice assistant", "when you use Meta AI"],
    "visit_rayban_website": ["when you visit our website", "when you browse our site"],
    "create_account": ["when you create an account", "when you sign up", "when you register"],
    # ...
}


This is fully transparent: you can show the lexicon to your professor.
________________


3. Split text into sentences / “clauses”
Using spaCy or NLTK to break each section into manageable units.
import spacy, json
from pathlib import Path


nlp = spacy.load("en_core_web_sm")  # non-LLM, just classic spaCy


def split_into_clauses(in_json, out_json):
    sections = json.loads(Path(in_json).read_text(encoding="utf-8"))
    clauses = []
    clause_id = 0


    for sec in sections:
        doc = nlp(sec["text"])
        for sent in doc.sents:
            text = sent.text.strip()
            if not text:
                continue
            clauses.append({
                "clause_id": clause_id,
                "section": sec["section"],
                "text": text
            })
            clause_id += 1


    Path(out_json).write_text(json.dumps(clauses, indent=2), encoding="utf-8")


# Example
split_into_clauses(
    "data/clean/rayban_privacy_2021-09-27.json",
    "data/clauses/rayban_privacy_2021-09-27_clauses.json"
)


Now you’re at the level: one sentence = one potential claim.
________________


4. Rule-based tagging: find activities and data in each clause
a) Simple keyword matching (baseline)
Start with lower-case string contains:
def find_tags(text, lexicon):
    text_l = text.lower()
    tags = set()
    for tag, keywords in lexicon.items():
        for kw in keywords:
            if kw in text_l:
                tags.add(tag)
                break
    return sorted(tags)


def tag_clause(clause, data_lex, activity_lex):
    text = clause["text"]
    data_tags = find_tags(text, data_lex)
    activity_tags = find_tags(text, activity_lex)
    return {
        **clause,
        "data_tags": data_tags,
        "activity_tags": activity_tags
    }


b) Use context to filter “real” collection statements
You don’t want every mention of “location” anywhere; usually you care about clauses that talk about collecting or using data:
Create a set of collection verbs and only keep clauses that mention at least one:
COLLECTION_VERBS = ["we collect", "we may collect", "we receive", "we obtain",
                    "we use information", "we process", "we store"]


def is_collection_clause(text):
    t = text.lower()
    return any(phrase in t for phrase in COLLECTION_VERBS)


def extract_claims(clauses, product, doc_id):
    claims = []
    for cl in clauses:
        if not is_collection_clause(cl["text"]):
            continue


        tagged = tag_clause(cl, DATA_LEXICON, ACTIVITY_LEXICON)
        if not tagged["data_tags"] and not tagged["activity_tags"]:
            continue


        claims.append({
            "doc_id": doc_id,
            "product": product,
            "section": tagged["section"],
            "clause_id": tagged["clause_id"],
            "activity": tagged["activity_tags"],
            "data_collected": tagged["data_tags"],
            "source_text": tagged["text"]
        })
    return claims


Run this for each policy version:
def run_pipeline_for_policy(clause_json, product, doc_id, out_path):
    clauses = json.loads(Path(clause_json).read_text(encoding="utf-8"))
    claims = extract_claims(clauses, product, doc_id)
    Path(out_path).write_text(json.dumps(claims, indent=2), encoding="utf-8")


If you want to be fancier, you can use spaCy’s Matcher to detect patterns like
“when you … we collect …” but even keyword rules already give you something concrete.
________________


6. Compare versions: what changed?
Once each policy version is converted into a list of claims, you can compare.
a) Normalize claims into keys
Make a tuple key like:
def claim_key(c):
    # For diffing, ignore clause_id; focus on what + when
    return (
        tuple(sorted(c["activity"])),
        tuple(sorted(c["data_collected"])),
        c["section"]
    )


b) Set operations between version A and B
import json


def load_claims(path):
    return json.loads(Path(path).read_text(encoding="utf-8"))


def diff_versions(v1_path, v2_path):
    v1 = load_claims(v1_path)
    v2 = load_claims(v2_path)


    v1_map = {claim_key(c): c for c in v1}
    v2_map = {claim_key(c): c for c in v2}


    keys1, keys2 = set(v1_map), set(v2_map)


    added = [v2_map[k] for k in keys2 - keys1]
    removed = [v1_map[k] for k in keys1 - keys2]
    kept = [v2_map[k] for k in keys1 & keys2]


    return added, removed, kept


Now you can say, for example:
         * In the new Meta glasses policy, there is a new claim:

            * Activity: use_voice_assistant

            * Data: audio, usage_events

               * Or: “rayban_site_2021” did not mention location, but “meta_glasses_2024” does.
You can present these differences in tables for your report.
________________


8. How to explain this to your professor (no LLM)
When you write it up, you can frame it like:
               1. Goal: Map activities to data types from multiple versions of Meta and Ray-Ban privacy policies.

               2. Data collection: Download official HTML policies; save versioned copies.

               3. Pre-processing:

                  * Strip HTML,

                  * Keep headings as sections,

                  * Split into sentences.

                     4. Ontology definition:

                        * Manually design a set of data categories and activities, motivated by manual reading + GDPR-style categories.

                           5. Rule-based NLP (Python, spaCy, regex, keyword lists):

                              * Identify clauses describing data collection (collection verbs),

                              * Tag them with data categories + activities using transparent keyword dictionaries.

                                 6. Structured output:

                                    * Export each clause into a JSON schema describing doc_id, section, activity, data types, and raw text.

                                       7. Change analysis:

                                          * Use set operations on claim keys to detect added/removed claims between policy versions.

                                             8. Cross-product comparison:

                                                * Compare claims for the same activity across Meta vs Ray-Ban ecosystem.
Everything is reproducible, inspectable, and explainable in code—no GPT or other LLM calls inside the pipeline.





Detailed Explanation
✅ STEP 0 — Define the Output Structure (“Schema”)
What Step 0 is doing
You design the data format that your final system will produce — basically the “shape” of each extracted claim.
Privacy policies are long, unstructured text. Your project wants to turn them into structured facts:
Activity → Data Being Collected → Purpose (optional)
 …• for each version of each policy
…• for each product (FB core, Ray-Ban website, Ray-Ban glasses, View App)
So Step 0 defines how one “fact” is represented in JSON, before you do any coding.
Why you’re doing Step 0
Because:
                                                * If you know the exact fields,

                                                * then Steps 1–7 can all focus on filling those fields.

Without Step 0, you would write code but not know exactly what to extract.
It makes the entire project precise and reproducible.
What rules Step 0 uses
The “rules” are your design choices, based on:
                                                   * What the professor expects (“What data is collected under what activity”)

                                                   * What is observable in the policies (Meta & Ray-Ban structure)

                                                   * What you will need for version-to-version comparison

Typically, you include:
Field
	Why this field is needed (rules)
	doc_id
	To distinguish different versions. (Rule: each policy version gets a unique ID.)
	section
	Because privacy policies are organized by headings that help you understand context.
	clause_id
	To keep track of the specific sentence you extracted.
	activity
	The “when” or “action” that triggers data collection → your main research question.
	data_collected
	The “what.” Also the main research question.
	source_text
	For auditability — professor can verify the extraction.
	Example schema (no code yet)
{
  "doc_id": "meta_fb_2024-09",
  "section": "Information we collect",
  "clause_id": 42,
  "activity": ["take_photo_with_glasses"],
  "data_collected": ["audio", "video_image", "device_info"],
  "source_text": "When you take photos and videos with the Ray-Ban Meta smart glasses, we collect audio and visual information and device logs."
}


Why this matters
This step forces you to think like a database designer:
                                                      * What exactly am I extracting?

                                                      * How will I compare policies?

                                                      * What fields will my code fill in?

________________


✅ STEP 1 — Collect and Normalize the Policies
Now that you know your output format, you need input:
                                                         * Different versions of Meta privacy policies

                                                         * Ray-Ban website privacy policy

                                                         * Ray-Ban glasses / View app policies

The raw HTML pages are messy and hard to analyze directly.
________________


STEP 1a — Download HTML from the Source
What this step is doing
This step fetches the raw HTML of the privacy policies and saves them into your own dataset folder.
You’re basically taking:
https://www.facebook.com/privacy/policy/version/8810742435690564


→ and downloading all the text behind it.
Why you're doing this
Because:
                                                            * You want reproducibility — if the policy changes later, you still have your saved copy.

                                                            * You need the HTML to extract text systematically.

                                                            * You want multiple versions to compare.

What rules this step uses
There isn’t “NLP rule” here — the rule is procedural:
                                                               1. Fetch only official URLs (never crawl random pages).

                                                               2. Store each version with a timestamp in filename
 e.g., meta_privacy_2024-10-01.html

                                                               3. Do NOT rely on web search; always fetch the given URLs.

This ensures:
                                                                  * Transparency

                                                                  * Academic reproducibility

                                                                  * Version tracking

How it fits into the workflow
All your later steps depend on having clean, versioned input.
________________


STEP 1b — Convert HTML → Structured Sections
Raw HTML contains:
                                                                     * <p>

                                                                     * <h1>, <h2>, <h3>

                                                                     * <ul>, <li>

                                                                     * <div> layout junk

                                                                     * menus and footers

So Step 1b extracts only the meaningful text.
What this step is doing
You extract:
                                                                        * Every heading (h1, h2, h3) — becomes section

                                                                        * Every paragraph or bullet — becomes text

And save it into:
[
  { "section": "Information we collect", "text": "We collect..." },
  { "section": "Information you provide", "text": "You give us..." },
  { "section": "Device information", "text": "We collect device..." }
]


Why you're doing this
Because:
                                                                           * Activities and data categories usually appear inside sections.

                                                                           * Sections help you contextualize the clause later.

                                                                           * You get a clean, consistent JSON format for NLP.

This also avoids needing to use LLMs — you get structure from HTML rules.
________________


What rules Step 1b uses
This step uses HTML structure rules, not NLP rules:
Rule 1 — Headings define sections
                                                                              * h1, h2, h3 tags are section titles

                                                                              * Every paragraph following a heading belongs to that section until the next heading

Rule 2 — Only keep content-bearing elements
You keep only:
                                                                                 * <p>

                                                                                 * <li>

                                                                                 * headings

You ignore:
                                                                                    * navigation bars

                                                                                    * footer links

                                                                                    * cookie pop-ups

                                                                                    * CSS, scripts, styles

Rule 3 — Preserve order
Order matters (policies are carefully written).
The JSON list respects the reading order of the original text.
Rule 4 — Normalize whitespace
You remove:
                                                                                       * HTML breaks

                                                                                       * extra newlines

                                                                                       * multi-space gaps

                                                                                       * unicode controls

The result is clean, NLP-friendly text.
________________


Why Step 1 is CRITICAL
Before you can:
                                                                                          * split into sentences

                                                                                          * detect activities

                                                                                          * detect data terms

                                                                                          * detect verbs like “we collect”

                                                                                          * compare versions

—you must have a clean, standardized, structured input.
Think of Step 1 as turning messy web pages into a structured dataset, like:
SECTION: "Information we collect"
TEXT: "When you use the Ray-Ban Meta glasses to take photos..."


From here, every later step is possible.
________________


⭐ Summary of Step 0 & 1
Step
	What it does
	Why
	Rules
	0. Define output schema
	Design the JSON structure that represents a “data collection claim.”
	You need a clear target format for later extraction.
	Follow logical fields: doc_id, section, activity, data_collected, raw text.
	1a. Download HTML
	Store raw versions of policies locally.
	Reproducibility; version comparison.
	Rule: fetch from official URLs; store with version tags.
	1b. Extract sections
	Clean the HTML → structured list of {section, text}
	Makes NLP possible; removes noise.
	Rules: (1) headings define sections, (2) keep only p/li/h1-h3, (3) preserve order, (4) normalize whitespace.
	

Big picture: what is Step 3?
You already have, from Step 1, a list like:
[
  { "section": "Information we collect", "text": "When you use the Ray-Ban Meta smart glasses... (long paragraph...)" },
  { "section": "How we use your information", "text": "We use the information we collect to..." }
]


Each text can be a long paragraph with multiple separate ideas (claims).
Step 3:
                                                                                             1. Breaks those paragraphs into smaller units (sentences / clauses),

                                                                                             2. So that each unit is a candidate “claim” like:
“When you take photos with the glasses, we collect audio and visual information.”

We’ll call each unit a clause (even though technically we start from sentences).
________________


3.1 What exactly is a “clause” here?
In your project, a clause is:
A short piece of text that could potentially express one data collection fact, such as:
                                                                                                * what data is collected

                                                                                                * when/under what activity

                                                                                                * maybe why (purpose)

Example from a policy:
“When you use the Ray-Ban Meta smart glasses to take photos and videos, we collect audio and visual information, as well as device log data.”
You want that whole sentence as one clause, because:
                                                                                                   * It has a clear trigger/activity: “use the glasses to take photos and videos”

                                                                                                   * It has clear data types: “audio and visual information, device log data”

Later, you’ll tag this clause with {activity: ..., data_collected: ...}.
So Step 3’s job is:
                                                                                                      * Turn unstructured paragraphs into a list of such short pieces, each with:

                                                                                                         * clause_id

                                                                                                         * section

                                                                                                         * text

________________


3.2 Why not just work with whole paragraphs?
Because paragraphs often mix multiple ideas:
“We collect information about how you use the glasses, including when you capture photos and videos, when you use voice commands, and when you pair the glasses with your phone. We may also collect information about your location if you enable location services.”
This has at least three activities:
                                                                                                            1. capture photos and videos

                                                                                                            2. use voice commands

                                                                                                            3. pair glasses with phone, plus

                                                                                                            4. separate idea: location if enabled

If you try to tag activities/data on the whole paragraph, you get:
                                                                                                               * messy associations (is location tied to all activities or only some?)

                                                                                                               * harder to debug / explain to your professor

Instead, by splitting into sentences/clauses, you isolate:
                                                                                                                  * “When you capture photos and videos…”

                                                                                                                  * “When you use voice commands…”

                                                                                                                  * “…when you pair the glasses…”

                                                                                                                  * “We may also collect information about your location if…”

Much simpler to tag.
________________


3.3 How do you split: practical rules
You can do this in two layers:
                                                                                                                     1. Sentence segmentation – use a standard library (e.g., spaCy)

                                                                                                                     2. (Optional) Sub-splitting very long sentences into smaller clauses

3.3.1 Sentence segmentation with spaCy (what it’s doing)
Code you saw earlier:
import spacy
nlp = spacy.load("en_core_web_sm")


def split_into_clauses(in_json, out_json):
    sections = json.loads(Path(in_json).read_text(encoding="utf-8"))
    clauses = []
    clause_id = 0


    for sec in sections:
        doc = nlp(sec["text"])
        for sent in doc.sents:
            text = sent.text.strip()
            if not text:
                continue
            clauses.append({
                "clause_id": clause_id,
                "section": sec["section"],
                "text": text
            })
            clause_id += 1


What spaCy does (conceptually)
spaCy’s sentence segmenter is not an LLM. It’s a rule-based + statistical component that:
                                                                                                                        * Looks for sentence boundaries (end of sentence punctuation: ., ?, !)

                                                                                                                        * Applies rules like:

                                                                                                                           * “Period + space + capital letter” → likely start of new sentence

                                                                                                                           * It knows common abbreviations like “e.g.”, “Mr.”, “Dr.” so it doesn’t split there

                                                                                                                              * It then returns doc.sents, which is a list of sentence objects.

So if your section text is:
When you use the Ray-Ban Meta smart glasses to take photos and videos, we collect audio and visual information, as well as device log data. We may also collect your location if you enable location services.


doc.sents becomes:
                                                                                                                                 1. "When you use the Ray-Ban Meta smart glasses to take photos and videos, we collect audio and visual information, as well as device log data."

                                                                                                                                 2. "We may also collect your location if you enable location services."

You then store each as a clause with its own clause_id.
What rules YOU add on top
In the code:
                                                                                                                                    * sent.text.strip() → remove leading/trailing whitespace

                                                                                                                                    * if not text: continue → drop empty strings

                                                                                                                                    * Assign clause_id by incrementing a counter → stable ID per clause

                                                                                                                                    * Keep track of which section each clause came from

So each clause is:
{
  "clause_id": 7,
  "section": "Information we collect",
  "text": "We may also collect your location if you enable location services."
}


Now each of these is your basic unit for tagging.
________________


3.3.2 Optional: splitting overly long sentences into smaller clauses
Policies love monster sentences joined with “and”, “or”, commas, and semicolons:
“When you use the Ray-Ban Meta smart glasses to take photos and videos, or use Meta AI to answer questions, or pair the glasses via Bluetooth to your mobile device, we collect device identifiers, log information and usage data.”
This might still be one sent from spaCy, but conceptually it’s multiple sub-activities.
You have two options:
Option A: Treat the whole thing as one clause
                                                                                                                                       * Simpler implementation.

                                                                                                                                       * You would end up tagging multiple activities in one clause:

                                                                                                                                          * ["take_photo_with_glasses", "use_voice_assistant", "pair_with_phone"]

                                                                                                                                             * This is fine if you’re okay with that granularity.

Option B: Add heuristic sub-splitting
You can further split based on connectors:
                                                                                                                                                * “or”

                                                                                                                                                * “and”

                                                                                                                                                * “;”

                                                                                                                                                * phrases like “when you”, “if you”

For example, a simple approach:
import re


def split_subclauses(sentence_text):
    # Example heuristic: split when "When you" or "If you" appears again
    parts = re.split(r'(?=(When you|If you|When using))', sentence_text)
    # This keeps separators; you'll have to recombine them sensibly.
    # Or simpler: split on ';'
    subclauses = []
    for part in sentence_text.split(';'):
        pt = part.strip()
        if pt:
            subclauses.append(pt)
    return subclauses


Conceptually, the rule here is:
If a sentence clearly contains multiple “when you…” patterns or multiple semicolon-separated ideas, consider splitting it so that each subclause corresponds more closely to a single activity.
This is optional and depends on your time/complexity budget.
________________


3.4 Rules for ignoring or cleaning clauses
Not every sentence is useful. You might want to filter out:
                                                                                                                                                   * Very generic introductory lines:
“This policy explains how we handle your information.”

                                                                                                                                                   * Section titles accidentally captured by spaCy (rare but possible)

                                                                                                                                                   * Very short, uninformative sentences:
“See below for more details.”

You can add rules like:
def is_candidate_clause(text):
    t = text.lower()
    # minimum length rule
    if len(t.split()) < 5:
        return False
    # ignore very generic disclaimers
    if "this policy explains" in t:
        return False
    if "for more information" in t and "visit" in t:
        return False
    return True


Then in your pipeline:
for sent in doc.sents:
    text = sent.text.strip()
    if not text or not is_candidate_clause(text):
        continue
    # add clause


This gives you a cleaner set of candidate clauses before tagging.


✅ STEP 4 — Rule-Based Tagging (Activities & Data Collected)
Step 4 takes each clause (from Step 3) and adds:
                                                                                                                                                      * data_collected → what types of data are mentioned

                                                                                                                                                      * activity → what the user is doing when data is collected

                                                                                                                                                      * optionally: purpose → why the data is collected

                                                                                                                                                      * direction → is this clause even talking about “collecting” data?

This is your “extraction logic” — entirely transparent, explainable, and code-based.
________________


4.0 Why rules instead of AI?
Your professor said:
                                                                                                                                                         * No black-box LLMs needed

                                                                                                                                                         * Python + simple NLP is OK

                                                                                                                                                         * Explainability is important

So Step 4 uses:
🟢 Keyword lexicons
 🟢 Pattern matching
 🟢 Explicit if–then rules
 🟢 Contextual filters (“only tag clauses that contain ‘we collect’”)
Very similar to how early information extraction systems worked.
________________


4.1 Inputs to Step 4
Each clause looks like:
{
  "clause_id": 7,
  "section": "Information we collect",
  "text": "When you take photos and videos with the Ray-Ban Meta smart glasses, we collect audio and visual information and device logs."
}


We want to turn it into:
{
  "clause_id": 7,
  "section": "Information we collect",
  "activity": ["take_photo_with_glasses"],
  "data_collected": ["audio", "video_image", "device_info"],
  "source_text": "...",
  "doc_id": "...",
  "product": "..."
}


________________


4.2 Overview of Step 4 Logic
Step 4 is composed of four substeps:
4A — Determine if this clause is actually about data collection
We check for “collection verbs.”
4B — Identify data types
Keyword dictionary: audio, video, location, device info, biometric, etc.
4C — Identify user activities
Keyword dictionary: take photos, use voice assistant, pair glasses, visit website, etc.
4D — Build extracted claim
Combine all tags into a structured JSON object.
Let’s explain each with full detail.
________________


4A — Determine if a clause expresses data collection
What this step is doing
Before you tag anything, you need to decide:
                                                                                                                                                            * Is this clause actually describing data collection?

A clause like:
“The glasses come in several color options.”
…should not be processed further.
Why
It prevents false positives from paragraphs that mention “audio,” “camera,” “location,” etc., but not in a data-collection context.
Core rule: “collection verbs”
Create a list of phrases that indicate collecting/processing/storing data:
COLLECTION_VERBS = [
    "we collect",
    "we may collect",
    "we receive",
    "we obtain",
    "we acquire",
    "we gather",
    "we use information",
    "we process",
    "information we collect",
    "we log",
    "we record"
]


Rule:
A clause is considered a “collection clause” if it contains ANY of these expressions (case insensitive).
Example:
Clause:
“When you take photos and videos with the glasses, we collect audio and visual information.”
Contains “we collect” → valid.
Clause:
“You can take photos or videos by tapping the frame.”
No “we collect” → skip it.
Exact implementation
def is_collection_clause(text):
    t = text.lower()
    return any(phrase in t for phrase in COLLECTION_VERBS)


________________


4B — Identify data types being collected
What this step is doing
Identify what the company says it collects.
Privacy policies often say things like:
                                                                                                                                                               * “We collect audio”

                                                                                                                                                               * “We collect videos/photos”

                                                                                                                                                               * “We collect location information”

                                                                                                                                                               * “We collect device log information”

                                                                                                                                                               * “We collect contacts”

                                                                                                                                                               * “We collect personal identifiers like email and phone number”

Rule: keyword lexicon
You group keywords into logical categories:
DATA_LEXICON = {
    "audio": [
        "audio", "microphone", "voice recording", "sound"
    ],
    "video_image": [
        "photo", "photos", "picture", "pictures",
        "video", "videos", "image", "images",
        "visual information"
    ],
    "location": [
        "location", "gps", "geolocation", "where you are",
        "precise location", "approximate location"
    ],
    "device_info": [
        "device information", "device info", "hardware",
        "software version", "os version", "device identifiers",
        "bluetooth id", "usage logs", "log information",
        "battery level"
    ],
    "usage_events": [
        "usage", "how you use", "interactions", "events",
        "log data", "app activity"
    ],
    "identifiers": [
        "email", "phone number", "name", "ip address",
        "account information"
    ],
    "biometric": [
        "face recognition", "facial geometry", "iris",
        "voiceprint"
    ]
}


Rule for extraction
For each clause:
If a keyword appears, assign the corresponding data type.
Implementation:
def find_data_tags(text):
    text_l = text.lower()
    tags = []
    for tag, keywords in DATA_LEXICON.items():
        for kw in keywords:
            if kw in text_l:
                tags.append(tag)
                break
    return tags


Example
Clause:
“we collect audio and visual information and device logs.”
Matches:
                                                                                                                                                                  * “audio” → audio

                                                                                                                                                                  * “visual information” → video_image

                                                                                                                                                                  * “device logs” → device_info

________________


4C — Identify activity (when data is collected)
What this step is doing
Activities describe when the user is doing something:
                                                                                                                                                                     * Taking photos

                                                                                                                                                                     * Recording videos

                                                                                                                                                                     * Saying “Hey Meta”

                                                                                                                                                                     * Using voice assistant

                                                                                                                                                                     * Pairing with phone

                                                                                                                                                                     * Visiting website

                                                                                                                                                                     * Creating account

                                                                                                                                                                     * Contacting support

                                                                                                                                                                     * Using companion app (View)

                                                                                                                                                                     * Browsing product pages

                                                                                                                                                                     * Enabling location services

Rule: activity lexicon
ACTIVITY_LEXICON = {
    "take_photo_with_glasses": [
        "when you take photos",
        "when you capture photos",
        "when you take pictures",
        "when you use the camera on the glasses"
    ],
    "record_video_with_glasses": [
        "when you record video",
        "when you capture videos"
    ],
    "use_voice_assistant": [
        "when you say 'hey meta'",
        "when you use voice commands",
        "voice assistant",
        "meta ai"
    ],
    "pair_with_phone": [
        "pair the glasses",
        "pair with your phone",
        "bluetooth pairing",
        "connect via bluetooth"
    ],
    "visit_website": [
        "when you visit our website",
        "when you browse our site",
        "when you visit"
    ],
    "create_account": [
        "when you create an account",
        "when you sign up",
        "when you register"
    ],
    "enable_location": [
        "if you enable location services",
        "when you allow location"
    ]
}


Rule for extraction
Same as before:
def find_activity_tags(text):
    text_l = text.lower()
    tags = []
    for tag, keywords in ACTIVITY_LEXICON.items():
        for kw in keywords:
            if kw in text_l:
                tags.append(tag)
                break
    return tags


Example
Clause:
“When you say ‘Hey Meta’ to activate voice commands, we collect audio and device log information.”
Activity detected → use_voice_assistant
________________


4D — Assemble the final claim
Once you have:
                                                                                                                                                                        * is_collection_clause → True

                                                                                                                                                                        * data_tags → e.g. ["audio", "device_info"]

                                                                                                                                                                        * activity_tags → e.g. ["use_voice_assistant"]

You assemble:
{
  "doc_id": doc_id,
  "product": product,
  "section": clause["section"],
  "clause_id": clause["clause_id"],
  "activity": activity_tags,
  "data_collected": data_tags,
  "source_text": clause["text"]
}


This becomes one entry in your dataset.
Filtering rule
If a clause:
                                                                                                                                                                           * is NOT a collection clause → skip

                                                                                                                                                                           * has NO activity and NO data → skip

Because it doesn’t help answer your research question.
Implementation
def extract_claims(clauses, product, doc_id):
    claims = []
    for cl in clauses:
        text = cl["text"]


        # A. Must talk about data collection
        if not is_collection_clause(text):
            continue


        # B. Extract data types
        data_tags = find_data_tags(text)


        # C. Extract activities
        activity_tags = find_activity_tags(text)


        # D. Only keep clauses with real signal
        if not data_tags and not activity_tags:
            continue


        # Build claim
        claims.append({
            "doc_id": doc_id,
            "product": product,
            "section": cl["section"],
            "clause_id": cl["clause_id"],
            "activity": activity_tags,
            "data_collected": data_tags,
            "source_text": text
        })


    return claims




Readme
Privacy Policy Analyzer (Rule-Based, Explainable System)
This repository contains the implementation of an explainable, rule-based system for analyzing privacy policies from Meta and Ray-Ban smart glasses.
The system identifies what types of data are collected and under which user activities, using only transparent and deterministic rules (no black-box LLMs).
The pipeline performs HTML cleaning, clause segmentation, rule-based extraction, and cross-version comparison of data-collection statements.
________________


⭐ Key Features
                                                                                                                                                                              * HTML → Cleaned Text
 Removes navigation bars, menus, scripts, ads, cookie banners, and other irrelevant HTML.

                                                                                                                                                                              * Clause Segmentation
 spaCy + heuristics to split long legal sentences into meaningful clauses.

                                                                                                                                                                              * Rule-Based Extraction

                                                                                                                                                                                 * Detects data-collection verbs

                                                                                                                                                                                 * Identifies data categories

                                                                                                                                                                                 * Identifies user activities

                                                                                                                                                                                 * Builds structured “claims” for each clause

                                                                                                                                                                                    * Cross-Version Comparison
 Computes added / removed / unchanged claims between policy versions.

                                                                                                                                                                                    * Fully Explainable
 No machine learning or LLMs; all rules are documented and reproducible.

________________


📁 Repository Structure
privacy-policy-analyzer/
│
├── README.md
├── requirements.txt
│
├── src/
│   ├── main.py
│   ├── html_cleaner.py
│   ├── clause_segmenter.py
│   ├── extractor.py
│   ├── rules.py
│   ├── compare_versions.py
│   └── utils.py
│
├── data/
│   ├── raw/           # raw HTML files
│   ├── cleaned/       # cleaned sections
│   └── examples/      # demonstration inputs/outputs
│
├── diagrams/
│   └── system_design.png
│
└── examples/
    ├── input.html
    ├── cleaned_section.json
    ├── segmented_clauses.json
    ├── extracted_claims.json
    └── version_diff.json


________________


⚙️ Installation
Install dependencies:
pip install -r requirements.txt


Download spaCy model if needed:
python -m spacy download en_core_web_sm


________________


▶️ Running the Pipeline
1. Analyze a policy file
python src/main.py --input data/raw/policy.html


This generates:
                                                                                                                                                                                       * cleaned text

                                                                                                                                                                                       * segmented clauses

                                                                                                                                                                                       * extracted claims JSON

Outputs are stored in data/cleaned/ and data/examples/.
________________


2. Compare two policy versions
python src/compare_versions.py \
    --old data/examples/v2023.json \
    --new data/examples/v2024.json


Produces:
{
  "added": [...],
  "removed": [...],
  "unchanged": [...]
}


________________


📌 Example Input/Output
Input HTML Snippet
<h2>Information we collect</h2>
<p>When you use voice commands, we collect audio data.</p>


Output Claim
{
  "section": "Information we collect",
  "activity": ["use_voice_assistant"],
  "data": ["audio"],
  "source_text": "When you use voice commands, we collect audio data."
}


________________


🧩 System Diagram
A high-level overview of the system pipeline.
diagrams/system_design.png


________________


📝 Rule-Based Approach
The full rule table (collection verbs, activity keywords, data-category tokens)
is included in rules.py and documented in the Methodology section of the report.