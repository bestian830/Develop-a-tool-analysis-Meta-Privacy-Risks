"""
ä¸šåŠ¡é€»è¾‘æœåŠ¡
"""
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from database import db_session
from models import PolicyAnalysis, PolicyComparison
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))
sys.path.insert(0, str(Path(__file__).parent.parent / 'tools'))
from analyzer import PrivacyPolicyAnalyzer
from compare_versions import PolicyVersionComparator
import re


class PolicyService:
    """éšç§æ”¿ç­–æœåŠ¡"""
    
    def __init__(self):
        self.analyzer = PrivacyPolicyAnalyzer()
        self.comparator = PolicyVersionComparator()
    
    def fetch_policy_content(self, url: str, use_selenium: bool = False) -> str:
        """
        çˆ¬å–éšç§æ”¿ç­–å†…å®¹
        
        å‚æ•°:
            url: éšç§æ”¿ç­–URL
            use_selenium: æ˜¯å¦ä½¿ç”¨Seleniumï¼ˆç”¨äºéœ€è¦JavaScriptæ¸²æŸ“çš„é¡µé¢ï¼‰
            
        è¿”å›:
            æ¸…ç†åçš„æ–‡æœ¬å†…å®¹
        """
        # å¦‚æœæ˜ç¡®è¦æ±‚ä½¿ç”¨Seleniumï¼Œç›´æ¥ä½¿ç”¨
        if use_selenium:
            return self._fetch_with_selenium(url)
        
        # å¦åˆ™å…ˆå°è¯•ä½¿ç”¨requestsæ–¹æ³•
        try:
            return self._fetch_with_requests(url)
        except Exception as e:
            # å¦‚æœrequestså¤±è´¥ä¸”å…è®¸ä½¿ç”¨Seleniumï¼Œå°è¯•Selenium
            error_msg = str(e)
            if "400" in error_msg or "403" in error_msg or "forbidden" in error_msg.lower() or "detected" in error_msg.lower():
                print(f"âš ï¸  Requestsæ–¹æ³•å¤±è´¥: {error_msg}")
                print("ğŸ”„ å°è¯•ä½¿ç”¨Seleniumæ–¹æ³•...")
                try:
                    return self._fetch_with_selenium(url)
                except Exception as selenium_error:
                    raise Exception(f"Requestsæ–¹æ³•å¤±è´¥: {error_msg}ã€‚Seleniumæ–¹æ³•ä¹Ÿå¤±è´¥: {str(selenium_error)}")
            else:
                raise Exception(f"Failed to fetch policy content: {str(e)}")
    
    def _fetch_with_requests(self, url: str) -> str:
        """ä½¿ç”¨requestsåº“çˆ¬å–ï¼ˆåŸºç¡€æ–¹æ³•ï¼‰"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',  # requestsä¼šè‡ªåŠ¨å¤„ç†gzip
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'DNT': '1',
            'Referer': 'https://www.google.com/',  # æ·»åŠ Refererï¼Œè®©å®ƒçœ‹èµ·æ¥åƒä»Googleè·³è½¬æ¥çš„
        }
        
        session = requests.Session()
        session.headers.update(headers)
        
        # å…è®¸é‡å®šå‘
        response = session.get(url, timeout=30, allow_redirects=True)
        
        # æ£€æŸ¥çŠ¶æ€ç 
        if response.status_code == 400:
            # Facebookè¿”å›400å¯èƒ½æ˜¯åçˆ¬è™«ï¼Œå°è¯•ä½¿ç”¨Selenium
            raise Exception(f"Bad Request (400). Facebook may have detected automated access. Try using Selenium method.")
        elif response.status_code == 403:
            raise Exception(f"Access forbidden (403). The website may have anti-scraping protection.")
        elif response.status_code == 404:
            raise Exception(f"Page not found (404). Please check if the URL is correct.")
        elif response.status_code >= 400:
            raise Exception(f"HTTP {response.status_code} error. The website may require authentication or have restrictions.")
        
        response.raise_for_status()
        
        # æ£€æŸ¥å†…å®¹ç±»å‹
        content_type = response.headers.get('Content-Type', '').lower()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯é¡µé¢
        response_text = response.text.lower()
        if 'error' in response_text[:500] and ('facebook' in response_text[:500] or 'not found' in response_text[:500]):
            raise Exception("Received error page instead of content. Facebook may have blocked the request.")
        
        # ä½¿ç”¨requestsè‡ªåŠ¨å¤„ç†çš„æ–‡æœ¬ï¼ˆå®ƒå·²ç»æ­£ç¡®å¤„ç†äº†ç¼–ç å’Œgzipï¼‰
        html_content = response.text
        
        # å¦‚æœresponse.textæœ‰é—®é¢˜ï¼Œæ‰‹åŠ¨å¤„ç†
        if not html_content or len(html_content) < 100:
            # æ‰‹åŠ¨å¤„ç†ç¼–ç 
            raw_content = response.content
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å‹ç¼©å†…å®¹ï¼ˆè™½ç„¶requestsåº”è¯¥å·²ç»å¤„ç†äº†ï¼‰
            content_encoding = response.headers.get('Content-Encoding', '').lower()
            if content_encoding == 'gzip':
                import gzip
                try:
                    raw_content = gzip.decompress(raw_content)
                except:
                    pass
            
            # æ£€æµ‹ç¼–ç 
            encoding = 'utf-8'
            if 'charset=' in content_type:
                try:
                    charset = content_type.split('charset=')[1].split(';')[0].strip()
                    if charset:
                        encoding = charset
                except:
                    pass
            
            # è§£ç 
            try:
                html_content = raw_content.decode(encoding, errors='replace')
            except:
                html_content = raw_content.decode('utf-8', errors='replace')
        
        if 'application/json' in content_type:
            # å¦‚æœæ˜¯JSONï¼Œå°è¯•è§£æ
            try:
                json_data = response.json()
                # å°è¯•æå–æ–‡æœ¬å†…å®¹
                if isinstance(json_data, dict):
                    # æŸ¥æ‰¾å¯èƒ½çš„æ–‡æœ¬å­—æ®µ
                    text_fields = ['content', 'text', 'body', 'html', 'data']
                    for field in text_fields:
                        if field in json_data:
                            return str(json_data[field])
                raise Exception("Received JSON response but couldn't extract text content")
            except:
                raise Exception("Received JSON response instead of HTML")
        
        # è§£æHTMLå¹¶æå–æ–‡æœ¬
        return self._extract_text_from_html(html_content)
    
    def _fetch_with_selenium(self, url: str) -> str:
        """ä½¿ç”¨Seleniumçˆ¬å–ï¼ˆç”¨äºéœ€è¦JavaScriptæ¸²æŸ“çš„é¡µé¢ï¼‰"""
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from selenium.webdriver.chrome.service import Service
            import time
        except ImportError:
            raise Exception("Seleniumæœªå®‰è£…ã€‚è¿è¡Œ: pip install selenium")
        
        # é…ç½®Chromeé€‰é¡¹
        chrome_options = Options()
        chrome_options.add_argument('--headless=new')  # ä½¿ç”¨æ–°çš„headlessæ¨¡å¼
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')  # éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        driver = None
        try:
            driver = webdriver.Chrome(options=chrome_options)
            
            # æ‰§è¡Œè„šæœ¬éšè—webdriverç‰¹å¾
            driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                '''
            })
            
            driver.get(url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(8)  # Facebooké¡µé¢å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´åŠ è½½
            
            # å°è¯•ç­‰å¾…ä¸»è¦å†…å®¹åŠ è½½
            try:
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
            except:
                pass
            
            # è·å–é¡µé¢å†…å®¹ - ä½¿ç”¨åŸæ¥çš„æ–¹æ³•ï¼šç›´æ¥è·å–bodyæ–‡æœ¬ï¼ˆSeleniumå·²ç»å¤„ç†äº†ç¼–ç ï¼‰
            try:
                # æ–¹æ³•1: ç›´æ¥è·å–bodyæ–‡æœ¬ï¼ˆæœ€ç®€å•å¯é ï¼ŒSeleniumå·²ç»å¤„ç†äº†ç¼–ç ï¼‰
                body_element = driver.find_element(By.TAG_NAME, "body")
                text_content = body_element.text
                
                # å¦‚æœè·å–çš„æ–‡æœ¬å¤ªçŸ­ï¼Œå¯èƒ½æ˜¯é¡µé¢è¿˜æ²¡åŠ è½½å®Œï¼Œå°è¯•è·å–HTML
                if len(text_content) < 500:
                    html_content = driver.page_source
                    # ç¡®ä¿HTMLå†…å®¹æ˜¯å­—ç¬¦ä¸²
                    if isinstance(html_content, bytes):
                        html_content = html_content.decode('utf-8', errors='replace')
                    # ä½¿ç”¨BeautifulSoupæå–
                    return self._extract_text_from_html(html_content)
                else:
                    # ç›´æ¥è¿”å›æ–‡æœ¬ï¼ˆå·²ç»æ¸…ç†è¿‡äº†ï¼‰
                    return text_content
            except Exception as e:
                # å¦‚æœç›´æ¥è·å–æ–‡æœ¬å¤±è´¥ï¼Œå›é€€åˆ°HTMLè§£æ
                html_content = driver.page_source
                # ç¡®ä¿HTMLå†…å®¹æ˜¯å­—ç¬¦ä¸²
                if isinstance(html_content, bytes):
                    html_content = html_content.decode('utf-8', errors='replace')
                # ä½¿ç”¨BeautifulSoupæå–
                return self._extract_text_from_html(html_content)
            
        except Exception as e:
            raise Exception(f"Seleniumçˆ¬å–å¤±è´¥: {str(e)}")
        finally:
            if driver:
                driver.quit()
    
    def _extract_text_from_html(self, html_content: str) -> str:
        """
        ä»HTMLå†…å®¹ä¸­æ™ºèƒ½æå–éšç§æ”¿ç­–æ–‡æœ¬ï¼ˆé¢„å¤„ç†é˜¶æ®µï¼‰
        
        ç­–ç•¥ï¼š
        1. ç§»é™¤è„šæœ¬å’Œæ ·å¼
        2. ç›´æ¥æå–bodyæ–‡æœ¬ï¼ˆæœ€ç®€å•å¯é ï¼‰
        3. åŸºæœ¬è¿‡æ»¤å™ªéŸ³å†…å®¹
        """
        # ç¡®ä¿HTMLå†…å®¹æ˜¯å­—ç¬¦ä¸²ä¸”ç¼–ç æ­£ç¡®
        if isinstance(html_content, bytes):
            try:
                html_content = html_content.decode('utf-8', errors='replace')
            except:
                html_content = html_content.decode('latin1', errors='replace').encode('utf-8', errors='replace').decode('utf-8', errors='replace')
        
        # ä½¿ç”¨lxmlè§£æå™¨ï¼Œå®ƒå¯¹ç¼–ç å¤„ç†æ›´å¥½
        # å¦‚æœlxmlä¸å¯ç”¨ï¼Œä½¿ç”¨html.parser
        try:
            soup = BeautifulSoup(html_content, 'lxml')
        except:
            # å¦‚æœlxmlå¤±è´¥ï¼Œä½¿ç”¨html.parser
            soup = BeautifulSoup(html_content, 'html.parser')
        
        # ç¬¬ä¸€æ­¥ï¼šåªç§»é™¤è„šæœ¬å’Œæ ·å¼ï¼ˆä¸è¦ç§»é™¤å…¶ä»–å…ƒç´ ï¼Œé¿å…ç ´åç»“æ„ï¼‰
        for tag in soup(["script", "style", "noscript", "iframe", "embed", "object"]):
            tag.decompose()
        
        # ç¬¬äºŒæ­¥ï¼šç›´æ¥ä½¿ç”¨bodyæå–æ–‡æœ¬ï¼ˆæœ€ç®€å•å¯é ï¼‰
        main_content = soup.find('body') or soup
        
        # ç¬¬ä¸‰æ­¥ï¼šç›´æ¥æå–æ–‡æœ¬ï¼ˆget_textå·²ç»æ­£ç¡®å¤„ç†äº†ç¼–ç ï¼‰
        text = main_content.get_text(separator='\n\n', strip=True)
        
        # ç¬¬å››æ­¥ï¼šåŸºæœ¬è¿‡æ»¤ï¼ˆåªè¿‡æ»¤æ˜æ˜¾çš„å™ªéŸ³ï¼‰
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        filtered_lines = []
        
        noise_keywords = {
            "click here", "learn more", "read more", "see more", "menu", "footer",
            "header", "navigation", "cookie settings", "settings", "home", "back",
            "next", "previous", "skip", "continue", "submit", "cancel", "close",
            "accept all", "reject all", "manage preferences", "sign in", "log in",
            "sign up", "register", "subscribe", "share", "print", "download",
            "search", "go", "ok", "yes", "no", "return to top", "back to top"
        }
        
        for line in lines:
            line_lower = line.lower().strip()
            
            # è·³è¿‡å¤ªçŸ­çš„è¡Œ
            if len(line_lower) < 10:
                continue
            
            # è·³è¿‡çº¯æ•°å­—æˆ–ç‰¹æ®Šå­—ç¬¦
            if not any(c.isalpha() for c in line_lower):
                continue
            
            # è·³è¿‡æ˜æ˜¾çš„å™ªéŸ³å…³é”®è¯ï¼ˆä½†ä¸è¦å¤ªä¸¥æ ¼ï¼Œé¿å…è¯¯åˆ ï¼‰
            if line_lower in noise_keywords and len(line_lower) < 50:
                continue
            
            filtered_lines.append(line)
        
        text = '\n\n'.join(filtered_lines)
        
        # ç¬¬äº”æ­¥ï¼šéªŒè¯å†…å®¹è´¨é‡
        if len(text) < 500:  # éšç§æ”¿ç­–é€šå¸¸è‡³å°‘500å­—ç¬¦
            raise Exception("Extracted content is too short. The page may require JavaScript to load content, or the URL doesn't contain a privacy policy.")
        
        return text
    
    def _remove_noise_elements(self, soup):
        """ç§»é™¤HTMLä¸­çš„å™ªéŸ³å…ƒç´ """
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for tag in soup(["script", "style", "noscript", "iframe", "embed", "object"]):
            tag.decompose()
        
        # ç§»é™¤å¯¼èˆªå’Œé¡µçœ‰é¡µè„š
        for tag in soup.find_all(["nav", "header", "footer", "aside"]):
            tag.decompose()
        
        # ç§»é™¤å¸¸è§çš„å¹¿å‘Šå’Œæ— å…³å†…å®¹å®¹å™¨
        noise_classes = [
            'advertisement', 'ad', 'ads', 'sidebar', 'menu', 'navigation',
            'cookie-banner', 'cookie-notice', 'popup', 'modal', 'overlay',
            'social-media', 'share-buttons', 'related-posts', 'comments',
            'breadcrumb', 'breadcrumbs', 'skip-link', 'skip-to-content'
        ]
        
        for class_name in noise_classes:
            for tag in soup.find_all(class_=lambda x: x and class_name in str(x).lower()):
                tag.decompose()
        
        # ç§»é™¤aria-labelåŒ…å«å¯¼èˆªã€èœå•ç­‰çš„å…ƒç´ 
        for tag in soup.find_all(attrs={"aria-label": lambda x: x and any(
            word in x.lower() for word in ['menu', 'navigation', 'skip', 'cookie']
        )}):
            tag.decompose()
        
        # ç§»é™¤roleä¸ºnavigation, banner, complementaryçš„å…ƒç´ 
        for role in ['navigation', 'banner', 'complementary', 'search']:
            for tag in soup.find_all(attrs={"role": role}):
                tag.decompose()
    
    def _find_policy_content(self, soup):
        """
        æ™ºèƒ½æŸ¥æ‰¾éšç§æ”¿ç­–çš„ä¸»è¦å†…å®¹åŒºåŸŸ
        
        ç­–ç•¥ï¼š
        1. æŸ¥æ‰¾åŒ…å«éšç§æ”¿ç­–å…³é”®è¯çš„å…ƒç´ 
        2. æŸ¥æ‰¾å¸¸è§çš„å†…å®¹å®¹å™¨
        3. é€‰æ‹©åŒ…å«æœ€å¤šæ”¿ç­–ç›¸å…³æ–‡æœ¬çš„åŒºåŸŸ
        """
        # éšç§æ”¿ç­–ç›¸å…³çš„å…³é”®è¯
        policy_keywords = [
            'privacy policy', 'privacy notice', 'data protection', 'personal information',
            'data collection', 'data use', 'data sharing', 'your rights', 'your data',
            'information we collect', 'how we use', 'third party', 'cookies',
            'gdpr', 'ccpa', 'pipeda', 'data retention', 'data security'
        ]
        
        # ä¼˜å…ˆçº§é€‰æ‹©å™¨ï¼ˆä»æœ€å…·ä½“åˆ°æœ€é€šç”¨ï¼‰
        selectors = [
            # æœ€å…·ä½“çš„éšç§æ”¿ç­–å®¹å™¨
            '[class*="privacy"]', '[class*="policy"]', '[id*="privacy"]', '[id*="policy"]',
            '[class*="legal"]', '[class*="terms"]',
            # é€šç”¨å†…å®¹å®¹å™¨
            'main', '[role="main"]', 'article', '[role="article"]',
            '.main-content', '.content', '.article-content', '.post-content',
            '.policy-content', '.legal-content', '#content', '#main', '#article'
        ]
        
        best_match = None
        best_score = 0
        
        # å°è¯•æ¯ä¸ªé€‰æ‹©å™¨
        for selector in selectors:
            try:
                elements = soup.select(selector)
                for element in elements:
                    text = element.get_text().lower()
                    # è®¡ç®—åŒ…å«æ”¿ç­–å…³é”®è¯çš„æ•°é‡
                    score = sum(1 for keyword in policy_keywords if keyword in text)
                    if score > best_score:
                        best_score = score
                        best_match = element
            except:
                continue
        
        # å¦‚æœæ‰¾åˆ°äº†åŒ…å«æ”¿ç­–å…³é”®è¯çš„åŒºåŸŸï¼Œè¿”å›å®ƒ
        if best_match and best_score >= 2:
            return best_match
        
        # å¦åˆ™å°è¯•æŸ¥æ‰¾åŒ…å«æœ€å¤šæ–‡æœ¬çš„articleæˆ–mainå…ƒç´ 
        for tag_name in ['article', 'main', 'div']:
            elements = soup.find_all(tag_name)
            for element in elements:
                text = element.get_text().strip()
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ”¿ç­–å…³é”®è¯ä¸”æ–‡æœ¬è¶³å¤Ÿé•¿
                if len(text) > 1000:
                    keyword_count = sum(1 for keyword in policy_keywords if keyword in text.lower())
                    if keyword_count >= 1:
                        return element
        
        return None
    
    def _extract_and_clean_text(self, element) -> str:
        """
        æå–æ–‡æœ¬å¹¶è¿›è¡Œé¢„å¤„ç†æ¸…ç†
        
        åœ¨çˆ¬å–é˜¶æ®µåªåšåŸºæœ¬æ¸…ç†ï¼Œä¿ç•™åŸå§‹æ–‡æœ¬å†…å®¹
        é¿å…è¿‡åº¦æ¸…ç†å¯¼è‡´ç¼–ç é—®é¢˜
        """
        if not element:
            return ""
        
        # æ–¹æ³•1: ç›´æ¥ä½¿ç”¨get_text()æå–ï¼ˆæœ€ç®€å•å¯é ï¼‰
        # è¿™å·²ç»æ­£ç¡®å¤„ç†äº†ç¼–ç 
        # ä½¿ç”¨separator='\n\n'æ¥ä¿æŒæ®µè½ç»“æ„
        text = element.get_text(separator='\n\n', strip=True)
        
        # å¦‚æœæå–çš„å†…å®¹å¤ªçŸ­ï¼Œå¯èƒ½æ˜¯æå–æ–¹æ³•æœ‰é—®é¢˜
        # å°è¯•æ›´ç»†è‡´çš„æå–
        if len(text) < 500:
            # å°è¯•æŒ‰æ®µè½æå–
            paragraphs = []
            for p in element.find_all(['p', 'div', 'section', 'article', 'main']):
                para_text = p.get_text(strip=True, separator=' ')
                # åªä¿ç•™æœ‰æ„ä¹‰çš„æ®µè½
                if para_text and len(para_text.strip()) > 20:
                    paragraphs.append(para_text)
            
            if paragraphs:
                text = '\n\n'.join(paragraphs)
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰è¶³å¤Ÿå†…å®¹ï¼Œç›´æ¥æå–æ‰€æœ‰æ–‡æœ¬
        if len(text) < 500:
            text = element.get_text(separator='\n\n')
        
        # è¿‡æ»¤å™ªéŸ³æ®µè½ï¼ˆåŸºäºå†…å®¹ï¼Œä¸åŸºäºå­—ç¬¦ç¼–ç ï¼‰
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        filtered_lines = []
        
        noise_keywords = {
            "click here", "learn more", "read more", "see more", "menu", "footer",
            "header", "navigation", "cookie settings", "settings", "home", "back",
            "next", "previous", "skip", "continue", "submit", "cancel", "close",
            "accept all", "reject all", "manage preferences", "sign in", "log in",
            "sign up", "register", "subscribe", "share", "print", "download",
            "search", "go", "ok", "yes", "no", "return to top", "back to top"
        }
        
        for line in lines:
            line_lower = line.lower().strip()
            
            # è·³è¿‡å¤ªçŸ­çš„è¡Œ
            if len(line_lower) < 10:
                continue
            
            # è·³è¿‡çº¯æ•°å­—æˆ–ç‰¹æ®Šå­—ç¬¦ï¼ˆä½†ä¿ç•™Unicodeå­—ç¬¦ï¼‰
            if not any(c.isalpha() for c in line_lower):
                continue
            
            # è·³è¿‡å™ªéŸ³å…³é”®è¯
            if line_lower in noise_keywords:
                continue
            
            # è·³è¿‡å¸¸è§çš„UIå…ƒç´ æ¨¡å¼
            if any(pattern in line_lower for pattern in [
                '^learn more', '^read more', '^click here', '^explore',
                '^back to top', '^table of contents', '^privacy center$'
            ]):
                continue
            
            # è·³è¿‡ç‰ˆæƒä¿¡æ¯ï¼ˆé€šå¸¸åœ¨é¡µè„šï¼‰
            if 'copyright' in line_lower or any(char in line for char in ['Â©', 'Â®', 'â„¢']):
                continue
            
            # è·³è¿‡çº¯é“¾æ¥æ–‡æœ¬ï¼ˆé€šå¸¸å¾ˆçŸ­ä¸”æ²¡æœ‰æ ‡ç‚¹ï¼‰
            if len(line.split()) <= 3 and not any(char in line for char in ['.', ',', ':', ';']):
                if line_lower.startswith(('http', 'www.', 'mailto:')):
                    continue
            
            filtered_lines.append(line)
        
        # åˆå¹¶è¡Œï¼Œç”¨åŒæ¢è¡Œåˆ†éš”
        return '\n\n'.join(filtered_lines)
    
    def _ensure_serializable(self, obj):
        """ç¡®ä¿å¯¹è±¡å¯ä»¥è¢«JSONåºåˆ—åŒ–ï¼ˆå¤„ç†setç­‰ç±»å‹ï¼‰"""
        import json
        try:
            # å…ˆå°è¯•åºåˆ—åŒ–ï¼Œå¦‚æœå¤±è´¥åˆ™é€’å½’å¤„ç†
            json.dumps(obj, ensure_ascii=False)
            return obj
        except (TypeError, ValueError):
            if isinstance(obj, dict):
                return {k: self._ensure_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, (list, tuple)):
                return [self._ensure_serializable(item) for item in obj]
            elif isinstance(obj, set):
                return list(obj)
            elif isinstance(obj, str):
                # ç¡®ä¿å­—ç¬¦ä¸²æ˜¯UTF-8ç¼–ç ï¼ˆä½†ä¸åšè¿‡åº¦æ¸…ç†ï¼‰
                try:
                    if isinstance(obj, bytes):
                        return obj.decode('utf-8', errors='replace')
                    # åªç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œä¸åšå†…å®¹æ¸…ç†
                    # æ¸…ç†å·¥ä½œç•™ç»™æ˜¾ç¤ºå±‚å¤„ç†
                    return str(obj)
                except:
                    return str(obj)
            elif hasattr(obj, '__dict__'):
                return self._ensure_serializable(obj.__dict__)
            else:
                return str(obj)
    
    def analyze_policy_from_url(self, url: str) -> dict:
        """
        ä»URLçˆ¬å–å¹¶åˆ†æéšç§æ”¿ç­–
        
        å‚æ•°:
            url: éšç§æ”¿ç­–URL
            
        è¿”å›:
            åˆ†æç»“æœå­—å…¸
        """
        # 1. çˆ¬å–å†…å®¹
        # å¯¹äºFacebookç­‰æœ‰åçˆ¬è™«ä¿æŠ¤çš„ç½‘ç«™ï¼Œç›´æ¥ä½¿ç”¨Selenium
        if 'facebook.com' in url or 'mbasic.facebook.com' in url:
            policy_content = self.fetch_policy_content(url, use_selenium=True)
        else:
            # å…¶ä»–ç½‘ç«™å…ˆå°è¯•requestsï¼Œå¤±è´¥åˆ™ä½¿ç”¨Selenium
            try:
                policy_content = self.fetch_policy_content(url, use_selenium=False)
            except Exception as e:
                # å¦‚æœrequestså¤±è´¥ï¼ˆç‰¹åˆ«æ˜¯400/403é”™è¯¯ï¼‰ï¼Œå°è¯•ä½¿ç”¨Selenium
                error_msg = str(e)
                if "400" in error_msg or "403" in error_msg or "forbidden" in error_msg.lower() or "detected" in error_msg.lower():
                    print(f"âš ï¸  Requestsæ–¹æ³•å¤±è´¥: {error_msg}")
                    print("ğŸ”„ å°è¯•ä½¿ç”¨Seleniumæ–¹æ³•...")
                    try:
                        policy_content = self.fetch_policy_content(url, use_selenium=True)
                    except Exception as selenium_error:
                        raise Exception(f"Requestsæ–¹æ³•å¤±è´¥: {error_msg}ã€‚Seleniumæ–¹æ³•ä¹Ÿå¤±è´¥: {str(selenium_error)}")
                else:
                    raise
        
        # 2. åˆ†æ
        analysis_result = self.analyzer.analyze(policy_content)
        
        # ç¡®ä¿åˆ†æç»“æœæ˜¯å¯åºåˆ—åŒ–çš„ï¼ˆå¤„ç†setç­‰ç±»å‹ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œåªåšåºåˆ—åŒ–å¤„ç†ï¼Œä¸åšæ–‡æœ¬æ¸…ç†ï¼ˆé¿å…ç ´ååŸå§‹å†…å®¹ï¼‰
        analysis_result = self._ensure_serializable(analysis_result)
        
        # 3. ä¿å­˜åˆ°æ•°æ®åº“
        summary = analysis_result.get('summary', {})
        policy_analysis = PolicyAnalysis(
            url=url,
            policy_content=policy_content,  # å­˜å‚¨åŸå§‹å†…å®¹ä½†ä¸è¿”å›
            analysis_result=analysis_result,
            total_segments=summary.get('total_segments', 0),
            average_risk_score=summary.get('average_risk_score', 0),
            total_data_types=len(summary.get('total_data_types', [])),
            total_third_parties=len(summary.get('total_third_parties', []))
        )
        
        db_session.add(policy_analysis)
        db_session.commit()
        
        # 4. è¿”å›ç»“æœï¼ˆä¸åŒ…å«åŸå§‹å†…å®¹ï¼‰
        return {
            'id': policy_analysis.id,
            'url': url,
            'analysis_result': analysis_result,
            'created_at': policy_analysis.created_at.isoformat() if policy_analysis.created_at else None
        }
    
    def get_all_reports(self) -> list:
        """è·å–æ‰€æœ‰åˆ†ææŠ¥å‘Šåˆ—è¡¨"""
        reports = db_session.query(PolicyAnalysis).order_by(PolicyAnalysis.created_at.desc()).all()
        return [report.to_dict() for report in reports]
    
    def get_report_by_id(self, report_id: int) -> dict:
        """æ ¹æ®IDè·å–åˆ†ææŠ¥å‘Š"""
        report = db_session.query(PolicyAnalysis).filter_by(id=report_id).first()
        if report:
            return report.to_dict()
        return None
    
    def delete_report(self, report_id: int) -> bool:
        """åˆ é™¤åˆ†ææŠ¥å‘Š"""
        report = db_session.query(PolicyAnalysis).filter_by(id=report_id).first()
        if report:
            db_session.delete(report)
            db_session.commit()
            return True
        return False
    
    def compare_policy_versions(self, old_url: str, new_url: str) -> dict:
        """
        å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬çš„éšç§æ”¿ç­–
        
        å‚æ•°:
            old_url: æ—§ç‰ˆæœ¬URL
            new_url: æ–°ç‰ˆæœ¬URL
            
        è¿”å›:
            å¯¹æ¯”ç»“æœå­—å…¸
        """
        # çˆ¬å–ä¸¤ä¸ªç‰ˆæœ¬çš„å†…å®¹
        old_content = self.fetch_policy_content(old_url, use_selenium=('facebook.com' in old_url or 'mbasic.facebook.com' in old_url))
        new_content = self.fetch_policy_content(new_url, use_selenium=('facebook.com' in new_url or 'mbasic.facebook.com' in new_url))
        
        # æ‰§è¡Œå¯¹æ¯”
        comparison_result = self.comparator.compare_versions(old_content, new_content)
        
        # ç¡®ä¿ç»“æœå¯åºåˆ—åŒ–
        comparison_result = self._ensure_serializable(comparison_result)
        
        return {
            'old_url': old_url,
            'new_url': new_url,
            'comparison_result': comparison_result
        }
    
    def compare_policy_texts(self, old_text: str, new_text: str) -> dict:
        """
        å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬çš„éšç§æ”¿ç­–æ–‡æœ¬
        
        å‚æ•°:
            old_text: æ—§ç‰ˆæœ¬æ–‡æœ¬
            new_text: æ–°ç‰ˆæœ¬æ–‡æœ¬
            
        è¿”å›:
            å¯¹æ¯”ç»“æœå­—å…¸
        """
        # æ‰§è¡Œå¯¹æ¯”
        comparison_result = self.comparator.compare_versions(old_text, new_text)
        
        # ç¡®ä¿ç»“æœå¯åºåˆ—åŒ–
        comparison_result = self._ensure_serializable(comparison_result)
        
        return {
            'comparison_result': comparison_result
        }
    
    def save_comparison(self, old_url: str = None, new_url: str = None, comparison_result: dict = None) -> dict:
        """
        ä¿å­˜ç‰ˆæœ¬å¯¹æ¯”ç»“æœåˆ°æ•°æ®åº“
        
        å‚æ•°:
            old_url: æ—§ç‰ˆæœ¬URLï¼ˆå¯é€‰ï¼‰
            new_url: æ–°ç‰ˆæœ¬URLï¼ˆå¯é€‰ï¼‰
            comparison_result: å¯¹æ¯”ç»“æœå­—å…¸
            
        è¿”å›:
            ä¿å­˜åçš„ç»“æœå­—å…¸ï¼ˆåŒ…å«IDï¼‰
        """
        if not comparison_result:
            raise ValueError("comparison_result is required")
        
        # æå–æ‘˜è¦ä¿¡æ¯
        risk_change_data = comparison_result.get('risk_change', {})
        
        # åˆ›å»ºå¯¹æ¯”è®°å½•
        comparison = PolicyComparison(
            old_url=old_url,
            new_url=new_url,
            comparison_result=comparison_result,
            risk_change=risk_change_data.get('risk_change', 0),
            old_average_risk=risk_change_data.get('old_average_risk', 0),
            new_average_risk=risk_change_data.get('new_average_risk', 0)
        )
        
        db_session.add(comparison)
        db_session.commit()
        
        return comparison.to_dict()
    
    def get_all_comparisons(self) -> list:
        """è·å–æ‰€æœ‰å¯¹æ¯”æŠ¥å‘Šåˆ—è¡¨"""
        comparisons = db_session.query(PolicyComparison).order_by(PolicyComparison.created_at.desc()).all()
        return [comp.to_dict() for comp in comparisons]
    
    def get_comparison_by_id(self, comparison_id: int) -> dict:
        """æ ¹æ®IDè·å–å¯¹æ¯”æŠ¥å‘Š"""
        comparison = db_session.query(PolicyComparison).filter_by(id=comparison_id).first()
        if comparison:
            return comparison.to_dict()
        return None
    
    def delete_comparison(self, comparison_id: int) -> bool:
        """åˆ é™¤å¯¹æ¯”æŠ¥å‘Š"""
        comparison = db_session.query(PolicyComparison).filter_by(id=comparison_id).first()
        if comparison:
            db_session.delete(comparison)
            db_session.commit()
            return True
        return False

